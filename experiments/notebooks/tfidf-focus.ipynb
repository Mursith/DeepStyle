{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupython --venv st-venv ~/notebooks/DeepLearning/asamin-dual.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 1:\n",
    "# screen -S asamin-dual1\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/misc-logs\n",
    "# DOCKER_PORT=9961 nn -o nohup-dual-$HOSTNAME-1.out ~/docker/keras/run-jupython.sh ~/notebooks/asa/eval/asamin-dual.ipynb titanv\n",
    "# observe ~/misc-logs/nohup-dual-$HOSTNAME-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 2:\n",
    "# screen -S asamin-dual2\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/misc-logs\n",
    "# DOCKER_PORT=9962 nn -o nohup-dual-$HOSTNAME-2.out ~/docker/keras/run-jupython.sh ~/notebooks/asa/eval/asamin-dual.ipynb titanv\n",
    "# observe ~/misc-logs/nohup-dual-$HOSTNAME-2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.seeder import *\n",
    "seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False # isNotebook, False, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newssource.asattribution.utils import *\n",
    "from newssource.asattribution.asamin import *\n",
    "from newssource.asa.asapreproc import *\n",
    "from newssource.asa.models import *\n",
    "from newssource.metrics.ndcg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if not isNotebook:\n",
    "    matplotlib.use('Agg')\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning.kerasutils import *\n",
    "from machinelearning.kerasmodels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning import kerasutils\n",
    "from machinelearning.iterator import *\n",
    "from machinelearning.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU, Dense, CuDNNLSTM, CuDNNGRU, TimeDistributed, Flatten, concatenate\n",
    "from keras.utils import multi_gpu_model, plot_model\n",
    "from keras.layers import concatenate, Input\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import Callback, History, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reloadKerasutils():\n",
    "    global kerasutils\n",
    "    kerasutils = reload(kerasutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir() + \"/asamin-dual.log\" if hjlat() else \"asamin-dual.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addResult(obj, *args, doPrint=True):\n",
    "    global config\n",
    "    global results\n",
    "    if isinstance(obj, float):\n",
    "        obj = {\"score\": obj}\n",
    "    if len(args) == 1:\n",
    "        if isinstance(args[0], list):\n",
    "            path = args[0]\n",
    "        else:\n",
    "            path = [args]\n",
    "    else:\n",
    "        path = args\n",
    "    currentResult = results\n",
    "    for key in path[:-1]:\n",
    "        if key not in currentResult:\n",
    "            currentResult[key] = dict()\n",
    "        currentResult = currentResult[key]\n",
    "    key = path[-1]\n",
    "    if key not in currentResult:\n",
    "        currentResult[key] = []\n",
    "    currentResult = currentResult[key]\n",
    "    localConf = copy.deepcopy(config)\n",
    "    for k in [\"doNotif\", \"doMultiGPU\", \"doFlattenSentences\", \"filterNonWordOrPunct\", \"punct\", \"outputDir\"]:\n",
    "        if k in localConf:\n",
    "            del localConf[k]\n",
    "    currentResult.append(mergeDicts(obj, {\"config\": localConf}))\n",
    "    if doPrint:\n",
    "        printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults():\n",
    "    global results\n",
    "    global logger\n",
    "    localRes = copy.deepcopy(results)\n",
    "    def recDel(d):\n",
    "        if isinstance(d, dict):\n",
    "            if \"config\" in d:\n",
    "                del d[\"config\"]\n",
    "            for key in d.keys():\n",
    "                d[key] = recDel(d[key])\n",
    "        elif isinstance(d, list):\n",
    "            for i in range(len(d)):\n",
    "                d[i] = recDel(d[i])\n",
    "        return d\n",
    "    localRes = recDel(localRes)\n",
    "    log(\"results without configs:\\n\" + lts(localRes), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveState():\n",
    "    toJsonFile(mongoStorable(config), config[\"outputDir\"] + \"/config.json\")\n",
    "    toJsonFile(results, config[\"outputDir\"] + \"/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = mergeDicts(config, \\\n",
    "{\n",
    "    # <IMPORTANT>\n",
    "    \"dataCol\": \"filtered_sentences\", # filtered_sentences, sentences, usetSentences, 3gramsFiltered, 2gramsFiltered, 1gramsFiltered, textSentences\n",
    "    \"minTokensLength\": 3,\n",
    "    \"minVocDF\": 2,\n",
    "    \"minVocLF\": 2,\n",
    "    \n",
    "    # <IMPORTANT>\n",
    "    # \"docLength\": 1200,\n",
    "    \n",
    "    # <IMPORTANT>\n",
    "    # \"wordVectorsPattern\": \"test\", # glove-6B, fasttext, glove-840B.300d\n",
    "    # \"embeddingsDimension\": 100,\n",
    "    # \"doLower\": True,\n",
    "    \n",
    "    # <IMPORTANT>\n",
    "    # ASA2:\n",
    "    \"transfer\": {\"dirName\": \"6ebdd3e05d4388c658ca2d5c53b0bc36\", \"epoch\": 35, \"aaPop\": 3, \"srLayerId\": -8}, # filtered_sentences, epochs 35 (???) 33 (???)\n",
    "    # \"transfer\": {\"dirName\": \"7fa22619af09e90724d2e3a8cf5db796\", \"epoch\": 26, \"aaPop\": 3, \"srLayerId\": -8}, # textSentences\n",
    "\n",
    "    # <OTHERS>\n",
    "    # \"scoring\": \"accuracy\",\n",
    "    # \"inputEncoding\": \"index\", # index, embedding # <IMPORTANT>\n",
    "    # \"cv\": 2 if TEST else 10,\n",
    "    # \"patience\": 0 if TEST else 30, # 30\n",
    "    # \"batchSize\": 32 if TEST else 32, # 128 doesn't work here, don't know why...\n",
    "    # \"doNotif\": not isNotebook,\n",
    "    \n",
    "    # uset0-l50-dpl50-d18-bc10, asa, bchack (others: blogcorpus, c50)\n",
    "    # uset0-l50-dpl200-d18-bc10, uset1-l50-dpl200-d18-bc10, uset2-l50-dpl200-d18-bc10, uset3-l50-dpl200-d18-bc10, uset4-l50-dpl200-d18-bc10\n",
    "    # uset0-l50-dpl160-blogger.com, uset1-l50-dpl160-blogger.com, uset2-l50-dpl160-blogger.com, uset3-l50-dpl160-blogger.com, uset4-l50-dpl160-blogger.com\n",
    "    \"datasetName\": \"uset4-l50-dpl160-blogger.com\",\n",
    "    \n",
    "    \n",
    "    \"usePretrained\": True,\n",
    "    \"batchSize\": 128,\n",
    "    \"patience\": 60,\n",
    "    \"epochs\": 500,\n",
    "    \"loss\": 'categorical_crossentropy', # sparse_categorical_crossentropy, categorical_crossentropy\n",
    "    \"metrics\": ['accuracy', 'top_k_categorical_accuracy'],\n",
    "    \"saveMetrics\":\n",
    "    {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_acc\": \"max\",\n",
    "        \"val_top_k_categorical_accuracy\": \"max\",\n",
    "    },\n",
    "    \"dropout\": 0.2,\n",
    "    \"denseUnits\": 100,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"outputDir\"] = nosaveDir() + \"/asa2-focus/\" + objectToHash(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    (dir, filename, ext, filenameExt) = decomposePath(config[\"outputDir\"])\n",
    "    config[\"outputDir\"] = dir + \"/tests/\" + filenameExt\n",
    "mkdir(config[\"outputDir\"])\n",
    "logger = Logger(config[\"outputDir\"] + \"/asa2-focus.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(lts(config), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRootDir = dataDir() + \"/Asa2/usets\"\n",
    "dataDirectory = dataRootDir + \"/\" + config[\"datasetName\"]\n",
    "asapFiles = sortedGlob(dataDirectory + \"/*.bz2\")\n",
    "bp(asapFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attTransferDir = nosaveDir() + \"/asa2-train/\" + config[\"transfer\"][\"dirName\"]\n",
    "attTransferEpochPath = attTransferDir + \"/models/epoch\" + digitalizeIntegers(str(config[\"transfer\"][\"epoch\"]), 4)\n",
    "attTransferWeightsPath = attTransferEpochPath + \"/weights.h5\"\n",
    "attTransferPrebuilt = deserialize(attTransferDir + \"/asap-prebuilt.pickle\")\n",
    "attTransferConfig = fromJsonFile(attTransferDir + \"/config.json\")\n",
    "attTransferDocLength = attTransferConfig[\"docLength\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asapKwargs = \\\n",
    "{\n",
    "    'dataCol': config[\"dataCol\"],\n",
    "    'minTokensLength': config[\"minTokensLength\"],\n",
    "    'minVocDF': config[\"minVocDF\"],\n",
    "    'minVocLF': config[\"minVocLF\"],\n",
    "    'persist': [True],\n",
    "    'docLength': attTransferConfig[\"docLength\"],\n",
    "    'labelEncoding': \"onehot\",\n",
    "    'logger': logger,\n",
    "}\n",
    "asap = buildASAP(asapFiles, **asapKwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attTransferPrebuilt[\"labelEncoder\"] = asap.labelEncoder\n",
    "attTransferPrebuilt[\"samplesCounts\"] = asap.samplesCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attTransferAsapKwargs = \\\n",
    "{\n",
    "    'dataCol': config[\"dataCol\"],\n",
    "    'minTokensLength': config[\"minTokensLength\"],\n",
    "    # 'batchSize': config[\"batchSize\"],\n",
    "    'minVocDF': config[\"minVocDF\"],\n",
    "    'minVocLF': config[\"minVocLF\"],\n",
    "    'persist': [True],\n",
    "    'prebuilt': attTransferPrebuilt,\n",
    "    'logger': logger,\n",
    "    'verbose': True,\n",
    "    \n",
    "    'docLength': attTransferDocLength,\n",
    "}\n",
    "attTransferAsapKwargs[\"encoding\"] = attTransferConfig[\"inputEncoding\"]\n",
    "attTransferAsap = buildASAP(asapFiles, **attTransferAsapKwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attTransferModelKwargs = fromJsonFile(attTransferEpochPath + \"/kwargs.json\")\n",
    "assert attTransferModelKwargs[\"vocSize\"] == len(attTransferPrebuilt[\"vocIndex\"])\n",
    "assert attTransferModelKwargs[\"docLength\"] == attTransferAsap.getDocLength()\n",
    "assert attTransferModelKwargs[\"vocSize\"] == len(attTransferAsap.getVocIndex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [row[\"filtered_sentences\"] for row in NDJson(asapFiles[0])]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitIndex = int(0.75 * len(docs))\n",
    "trainDocs = docs[:splitIndex]\n",
    "testDocs = docs[splitIndex:]\n",
    "log(str(len(trainDocs)) + \" docs in the training set and \" + str(len(testDocs)) + \" docs in the test set\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init of DeepStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepstyle.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelName = \"6ebdd3e05d4388c658ca2d5c53b0bc36\"\n",
    "modelName = \"extrafocus-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepStyle(nosaveDir() + \"/asa2-train/\" + modelName, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We get attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = model.attentions(testDocs, progressVerbose=True)\n",
    "# model.save()\n",
    "bp(attentions, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncateAttentions(attentions, docs):\n",
    "    newAttentions = []\n",
    "    for i in range(len(docs)):\n",
    "        # Getting the attention of the current doc:\n",
    "        doc = docs[i]\n",
    "        attention = attentions[i]\n",
    "        docLength = len(attention)\n",
    "        # Unpadding the attention:\n",
    "        if len(doc) < docLength:\n",
    "            pad = docLength - len(doc)\n",
    "            attention = attention[pad:]\n",
    "        # We check the shape:\n",
    "        if len(doc) < docLength:\n",
    "            assert len(attention) == len(doc)\n",
    "        else:\n",
    "            assert len(attention) == docLength\n",
    "        # Making it a proba distrib:\n",
    "        attention = toProbDist(attention)\n",
    "        # And finally add it to attentions:\n",
    "        newAttentions.append(attention)\n",
    "    return newAttentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attDocs = [x[0] for x in attTransferAsap.getRawPart(truncate=True, pad=False)]\n",
    "attTestDocs = attDocs[splitIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(attTestDocs) == len(attentions)\n",
    "attentions = truncateAttentions(attentions, attTestDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We get TFIDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfDocs = [flattenLists(doc)[:1200] for doc in docs]\n",
    "bp(tfidfDocs, 4, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TFIDF(tfidfDocs, logger=logger, doLower=True, sublinearTF=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfValues = tfidf.getTFIDFVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfValues = tfidfValues[splitIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tfidfValues)):\n",
    "    tfidfValues[i] = toProbDist(tfidfValues[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tfidfValues) == len(attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tfidfValues)):\n",
    "    assert len(tfidfValues[i]) == len(attentions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp(tfidfValues, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We get the attention focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttentionFocus(attentions, tfidfValues):\n",
    "    attentionFocus = []\n",
    "    for i in range(len(attentions)):\n",
    "        attention = attentions[i]\n",
    "        tfidf = tfidfValues[i]\n",
    "        attentionFocus.append(np.dot(attention, tfidf))\n",
    "    return np.mean(attentionFocus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionFocus = getAttentionFocus(attentions, tfidfValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionFocus = attentionFocus * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log(\"attention focus: \" + str(attentionFocus), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
