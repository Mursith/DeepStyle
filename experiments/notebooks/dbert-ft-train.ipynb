{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBert train\n",
    "\n",
    "From <https://github.com/huggingface/transformers/blob/master/examples/run_tf_glue.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanv tf install:\n",
    "# !pip freeze | grep flow\n",
    "# !pip install --upgrade pip\n",
    "# !pip uninstall --y tensorboard tensorflow-estimator tensorflow tensorflow-gpu\n",
    "# !pip install --upgrade tensorflow==2.0.0\n",
    "# !pip install --upgrade tensorflow-gpu==2.0.0\n",
    "# !pip install --upgrade transformers==2.4.1\n",
    "# !pip freeze | grep flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 1:\n",
    "# screen -S dbert-train1\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/dbert-train-logs\n",
    "# DOCKER_PORT=9961 nn -o nohup-dbert-train-$HOSTNAME-1.out ~/docker/keras/run-jupython.sh ~/notebooks/asa/train/dbert-train.ipynb titanv\n",
    "# observe ~/dbert-train-logs/nohup-dbert-train-$HOSTNAME-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 2:\n",
    "# screen -S dbert-train2\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/dbert-train-logs\n",
    "# DOCKER_PORT=9962 nn -o nohup-dbert-train-$HOSTNAME-2.out ~/docker/keras/run-jupython.sh ~/notebooks/asa/train/dbert-train.ipynb titanv\n",
    "# observe ~/dbert-train-logs/nohup-dbert-train-$HOSTNAME-2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ; archive-notebooks ; cd ~/logs ; ./mv-old-logs.sh # optionnel\n",
    "# sbatch ~/slurm/run-notebook.sh ~/tmp/archives/notebooks/asa/train/dbert-train.ipynb\n",
    "# observe ~/logs/*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ; archive-notebooks ; cd ~/logs\n",
    "# sbatch ~/slurm/run-notebook.sh ~/tmp/archives/notebooks/asa/train/dbert-train.ipynb\n",
    "# observe ~/logs/*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "from transformers import \\\n",
    "(\n",
    "    DistilBertConfig,\n",
    "    DistilBertTokenizer,\n",
    "    TFDistilBertForSequenceClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksetGen\\\n",
    "(\n",
    "    train=True,\n",
    "    ksetRoot=dataDir() + '/Asa2/detok-kset' if lri() else homeDir() + \"/asa/asa2-data/detok-kset\",\n",
    "    maxFiles=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    # We find files:\n",
    "    if train:\n",
    "        files = sortedGlob(ksetRoot + '/train/*.bz2')\n",
    "    else:\n",
    "        files = sortedGlob(ksetRoot + '/validation/*.bz2')\n",
    "    if maxFiles is not None:\n",
    "        files = files[:maxFiles]\n",
    "    # we return the generator:\n",
    "    return genFunct(files, ksetRoot=ksetRoot, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFunct\\\n",
    "(\n",
    "    files,\n",
    "    \n",
    "    ksetRoot=dataDir() + '/Asa2/detok-kset' if lri() else homeDir() + \"/asa/asa2-data/detok-kset\",\n",
    "    dataCol=\"filtered_detokenized_sentences\",\n",
    "    labelField='label',\n",
    "    \n",
    "    labelEncoding='index',\n",
    "    labelEncoder=None,\n",
    "    \n",
    "    maxSamples=None,\n",
    "    maxSentences=None,\n",
    "    \n",
    "    preventTokenizerWarnings=True,\n",
    "    loggerName=\"transformers.tokenization_utils\",\n",
    "    \n",
    "    logger=None,\n",
    "    verbose=True,\n",
    "    \n",
    "    showProgress=False,\n",
    "    \n",
    "    multiSamplage=False,\n",
    "    **encodeKwargs,\n",
    "):\n",
    "    # Handling unique file:\n",
    "    if not isinstance(files, list):\n",
    "        files = [files]\n",
    "    # Misc init:\n",
    "    samplesCount = 0\n",
    "    # We set the logger level:\n",
    "    if preventTokenizerWarnings:\n",
    "        previousLoggerLevel = logging.getLogger(loggerName).level\n",
    "        logging.getLogger(loggerName).setLevel(logging.ERROR)\n",
    "    if showProgress:\n",
    "        pbar = ProgressBar(len(files), logger=logger, verbose=verbose)\n",
    "    # We get labels and encode labels:\n",
    "    if labelEncoder is None:\n",
    "        labels = sorted(list(deserialize(ksetRoot + '/validation/labels.pickle')))\n",
    "        (classes, labels) = encodeMulticlassLabels(labels, encoding=labelEncoding)\n",
    "        labelEncoder = dict()\n",
    "        assert len(classes) == len(labels)\n",
    "        for i in range(len(classes)):\n",
    "            labelEncoder[classes[i]] = labels[i]\n",
    "    # For each file:\n",
    "    for file in files:\n",
    "        for row in NDJson(file):\n",
    "            # We get sentences:\n",
    "            sentences = row[dataCol]\n",
    "            if not (isinstance(sentences, list) and len(sentences) > 1 and isinstance(sentences[0], str)):\n",
    "                raise Exception(\"All row[dataCol] must be a list of strings (sentences)\")\n",
    "            if maxSentences is not None:\n",
    "                sentences = sentences[:maxSentences]\n",
    "            # We encode the document:\n",
    "            parts = tf2utils.distilBertEncode\\\n",
    "            (\n",
    "                sentences,\n",
    "                multiSamplage=multiSamplage,\n",
    "                preventTokenizerWarnings=False,\n",
    "                proxies=proxies,\n",
    "                logger=logger, verbose=verbose,\n",
    "                **encodeKwargs,\n",
    "            )\n",
    "            if not multiSamplage:\n",
    "                parts = [parts]\n",
    "            # We yield all parts:\n",
    "            for part in parts:\n",
    "                yield (np.array(part), labelEncoder[row[labelField]])\n",
    "                # yield (np.array([np.array(part), np.array(part)]), np.array([labelEncoder[row[labelField]], labelEncoder[row[labelField]]]))\n",
    "                samplesCount += 1\n",
    "            if maxSamples is not None and samplesCount >= maxSamples:\n",
    "                break\n",
    "        if showProgress:\n",
    "            pbar.tic(file)\n",
    "        if maxSamples is not None and samplesCount >= maxSamples:\n",
    "            break\n",
    "    # We reset the logger:\n",
    "    if preventTokenizerWarnings:\n",
    "        logging.getLogger(loggerName).setLevel(previousLoggerLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFunct(model, directory, **kwargs):\n",
    "    model.save_pretrained(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamplesCount(logger=None, verbose=True):\n",
    "    samplesCountCache = None\n",
    "    (user, password, host) = getOctodsMongoAuth()\n",
    "    samplesCountCache = SerializableDict('samples-count', user=user, host=host, password=password, useMongodb=True)\n",
    "    samplesCountParams = \\\n",
    "    {\n",
    "        'maxFiles': config['maxFiles'],\n",
    "        'maxSamples': config['maxSamples'],\n",
    "        'multiSamplage': config['multiSamplage'],\n",
    "        'maxLength': config['maxLength'],\n",
    "        'dataCol': config['dataCol'],\n",
    "    }\n",
    "    trainSamplesCountParams = mergeDicts(samplesCountParams, {'train': True})\n",
    "    trainSamplesCountHash = objectToHash(trainSamplesCountParams)\n",
    "    validationSamplesCountParams = mergeDicts(samplesCountParams, {'train': False})\n",
    "    validationSamplesCountHash = objectToHash(validationSamplesCountParams)\n",
    "    if samplesCountCache is not None and trainSamplesCountHash in samplesCountCache:\n",
    "        trainSamplesCount = samplesCountCache[trainSamplesCountHash]\n",
    "    else:\n",
    "        log(\"Starting to count batches in the train set...\", logger, verbose=verbose)\n",
    "        trainSamplesCount = 0\n",
    "        for row in ksetGen\\\n",
    "        (\n",
    "            train=True,\n",
    "            **samplesCountParams,\n",
    "            showProgress=True,\n",
    "            logger=logger,\n",
    "            verbose=True,\n",
    "        ):\n",
    "            trainSamplesCount += 1\n",
    "        if samplesCountCache is not None:\n",
    "            samplesCountCache[trainSamplesCountHash] = trainSamplesCount\n",
    "    if samplesCountCache is not None and validationSamplesCountHash in samplesCountCache:\n",
    "        validationSamplesCount = samplesCountCache[validationSamplesCountHash]\n",
    "    else:\n",
    "        log(\"Starting to count batches in the validation set...\", logger, verbose=verbose)\n",
    "        validationSamplesCount = 0\n",
    "        for row in ksetGen\\\n",
    "        (\n",
    "            train=False,\n",
    "            **samplesCountParams,\n",
    "            showProgress=True,\n",
    "            logger=logger,\n",
    "            verbose=True,\n",
    "        ):\n",
    "            validationSamplesCount += 1\n",
    "        if samplesCountCache is not None:\n",
    "            samplesCountCache[validationSamplesCountHash] = validationSamplesCount\n",
    "    return (trainSamplesCount, validationSamplesCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'dataCol': 'filtered_detokenized_sentences',\n",
    "    'ksetRoot': dataDir() + '/Asa2/detok-kset' if lri() else homeDir() + \"/asa/asa2-data/detok-kset\",\n",
    "    'multiSamplage': True,\n",
    "    'maxFiles': 30 if isNotebook else None,\n",
    "    'maxSamples': 5000 if isNotebook else None,\n",
    "    'maxLength': 512,\n",
    "    'batchSize': 16,\n",
    "    \n",
    "    'learningRate': 3e-5,\n",
    "    'epsilon': 1e-08,\n",
    "    'clipnorm': 1.0,\n",
    "    \n",
    "    'trainStepDivider': 2 if isNotebook else 30,\n",
    "    'shuffle': 0 if isNotebook else 100,\n",
    "    'queueSize': 100,\n",
    "    \n",
    "    'useMLIterator': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksetRoot = config['ksetRoot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDirRoot = homeDir() + '/asa/dbert-train'\n",
    "outputDir = outputDirRoot + '/' + objectToHash(config)[:5]\n",
    "mkdir(outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    assert config['maxFiles'] == 3\n",
    "    assert isNotebook\n",
    "    remove(outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(outputDir + '/dbert-train.log')\n",
    "log(\"outputDir: \" + str(outputDir), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFiles = sortedGlob(ksetRoot + '/train/*.bz2')\n",
    "validationFiles = sortedGlob(ksetRoot + '/validation/*.bz2')\n",
    "if config['maxFiles'] is not None:\n",
    "    log(\"Reducing amount of train files from \" + str(len(trainFiles)) + \" to \" + str(config['maxFiles']), logger)\n",
    "    trainFiles = trainFiles[:config['maxFiles']]\n",
    "    log(\"Reducing amount of validation files from \" + str(len(validationFiles)) + \" to \" + str(config['maxFiles']), logger)\n",
    "    validationFiles = validationFiles[:config['maxFiles']]\n",
    "bp(trainFiles, logger)\n",
    "bp(validationFiles, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we reume a previous train:\n",
    "batchesPassed = 0\n",
    "initialEpoch = 0\n",
    "lastEpochPath = None\n",
    "if len(sortedGlob(outputDir + \"/epochs/ep*\")) > 0:\n",
    "    lastEpochPath = sortedGlob(outputDir + \"/epochs/ep*\")[-1]\n",
    "    batchesPassedPath = lastEpochPath + \"/batchesPassed.txt\"\n",
    "    assert isFile(batchesPassedPath)\n",
    "    assert not isFile(outputDir + \"/finished\")\n",
    "    assert not isFile(outputDir + \"/stop\")\n",
    "    initialEpoch = getFirstNumber(decomposePath(lastEpochPath)[1]) + 1\n",
    "    batchesPassed = int(fileToStr(batchesPassedPath))\n",
    "    log(\"We found an epoch to resume: \" + lastEpochPath, logger)\n",
    "    logWarning(\"We will skip \" + str(batchesPassed) + \" batches because we resume a previous train\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lastEpochPath is not None:\n",
    "    log(\"Loading previous model...\", logger)\n",
    "    dbertConfig = DistilBertConfig.from_pretrained(lastEpochPath + '/config.json')\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained\\\n",
    "    (\n",
    "        lastEpochPath + '/tf_model.h5',\n",
    "        config=dbertConfig,\n",
    "    )\n",
    "else:\n",
    "    log(\"Loading a new model from distilbert-base-uncased...\", logger)\n",
    "    # Labels count:\n",
    "    numLabels = len(deserialize(ksetRoot + '/validation/labels.pickle'))\n",
    "    # Config:\n",
    "    dbertConfig = DistilBertConfig.from_pretrained\\\n",
    "    (\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=numLabels,\n",
    "        max_length=config['maxLength'],\n",
    "        proxies=proxies,\n",
    "    )\n",
    "    # Model:\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained\\\n",
    "    (\n",
    "        \"distilbert-base-uncased\",\n",
    "        config=dbertConfig,\n",
    "        proxies=proxies,\n",
    "    )\n",
    "log(\"Model loaded.\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer:\n",
    "optKwargs = dict()\n",
    "if dictContains(config, 'clipnorm'): optKwargs['clipnorm'] = config['clipnorm']\n",
    "if dictContains(config, 'learningRate'): optKwargs['learning_rate'] = config['learningRate']\n",
    "if dictContains(config, 'epsilon'): optKwargs['epsilon'] = config['epsilon']\n",
    "opt = tf.keras.optimizers.Adam(**optKwargs)\n",
    "# Loss:\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Metric:\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "# Compilation:\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainSamplesCount, validationSamplesCount) = getSamplesCount(logger=logger)\n",
    "log('trainSamplesCount: ' + str(trainSamplesCount) + ', validationSamplesCount: ' + str(validationSamplesCount), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBatchesAmount = math.ceil(trainSamplesCount / config['batchSize'])\n",
    "validationBatchesAmount = math.ceil(validationSamplesCount / config['batchSize'])\n",
    "trainSteps = math.ceil(trainBatchesAmount / config[\"trainStepDivider\"])\n",
    "validationSteps = validationBatchesAmount\n",
    "log('trainBatchesAmount: ' + str(trainBatchesAmount), logger)\n",
    "log('validationBatchesAmount: ' + str(validationBatchesAmount), logger)\n",
    "log('trainSteps: ' + str(trainSteps), logger)\n",
    "log('validationSteps: ' + str(validationSteps), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf2utils.KerasCallback\\\n",
    "(\n",
    "    model,\n",
    "    outputDir,\n",
    "    saveFunct=saveFunct,\n",
    "    showGraphs=isNotebook,\n",
    "    earlyStopMonitor=\n",
    "    {\n",
    "        'val_loss': {'patience': 10, 'mode': 'auto'},\n",
    "        'val_accuracy': {'patience': 10, 'mode': 'auto'},\n",
    "        'val_top_k_categorical_accuracy': {'patience': 10, 'mode': 'auto'},\n",
    "    },\n",
    "    initialEpoch=initialEpoch,\n",
    "    batchesAmount=trainBatchesAmount,\n",
    "    batchesPassed=batchesPassed,\n",
    "    removeEpochs=True,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksetGenKwargs = \\\n",
    "{\n",
    "    'ksetRoot': ksetRoot,\n",
    "    'dataCol': config['dataCol'],\n",
    "    'maxLength': config['maxLength'],\n",
    "    'multiSamplage': config['multiSamplage'],\n",
    "    'maxSamples': config['maxSamples'],\n",
    "}\n",
    "ksetGenTrainKwargs = mergeDicts(ksetGenKwargs, {'train': True})\n",
    "ksetGenValidationKwargs = mergeDicts(ksetGenKwargs, {'train': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if config['useMLIterator']:\n",
    "    train = IteratorToGenerator\\\n",
    "    (\n",
    "        InfiniteBatcher\\\n",
    "        (\n",
    "            AgainAndAgain\\\n",
    "            (\n",
    "                MLIterator,\n",
    "                trainFiles,\n",
    "                genFunct,\n",
    "                genKwargs=ksetGenKwargs,\n",
    "                queuesMaxSize=100,\n",
    "                parallelProcesses=cpuCount(),\n",
    "                useFlushTimer=False,\n",
    "                flushTimeout=300,\n",
    "                logger=logger,\n",
    "            ),\n",
    "            batchSize=config['batchSize'],\n",
    "            shuffle=config['shuffle'],\n",
    "            queueSize=config['queueSize'],\n",
    "            skip=batchesPassed,\n",
    "            logger=logger,\n",
    "        )\n",
    "    )\n",
    "    validation = IteratorToGenerator\\\n",
    "    (\n",
    "        InfiniteBatcher\\\n",
    "        (\n",
    "            AgainAndAgain\\\n",
    "            (\n",
    "                MLIterator,\n",
    "                validationFiles,\n",
    "                genFunct,\n",
    "                genKwargs=ksetGenKwargs,\n",
    "                queuesMaxSize=100,\n",
    "                parallelProcesses=cpuCount(),\n",
    "                useFlushTimer=False,\n",
    "                flushTimeout=300,\n",
    "                logger=logger,\n",
    "            ),\n",
    "            batchSize=config['batchSize'],\n",
    "            shuffle=config['shuffle'],\n",
    "            queueSize=config['queueSize'],\n",
    "            skip=batchesPassed,\n",
    "            logger=logger,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    train = IteratorToGenerator(InfiniteBatcher\\\n",
    "    (\n",
    "        AgainAndAgain(ksetGen, **ksetGenTrainKwargs),\n",
    "        batchSize=config['batchSize'],\n",
    "        shuffle=config['shuffle'],\n",
    "        queueSize=config['queueSize'],\n",
    "        skip=batchesPassed,\n",
    "        logger=logger,\n",
    "    ))\n",
    "    validation = IteratorToGenerator(InfiniteBatcher\\\n",
    "    (\n",
    "        AgainAndAgain(ksetGen, **ksetGenValidationKwargs),\n",
    "        batchSize=config['batchSize'],\n",
    "        shuffle=0,\n",
    "        queueSize=100,\n",
    "        skip=0,\n",
    "        logger=logger,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit\\\n",
    "(\n",
    "    x=train,\n",
    "    epochs=100 * config[\"trainStepDivider\"],\n",
    "    validation_data=validation,\n",
    "    callbacks=[callback, callbacks.TerminateOnNaN()],\n",
    "    initial_epoch=initialEpoch,\n",
    "    steps_per_epoch=trainSteps,\n",
    "    validation_steps=validationSteps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
