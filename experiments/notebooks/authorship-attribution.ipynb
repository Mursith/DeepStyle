{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ~/asa2-aa-logs ; jupython --no-tail --venv st-venv -o nohup-asa2-aa-$HOSTNAME.out ~/notebooks/asa/eval/asa2-aa.ipynb\n",
    "# observe ~/asa2-aa-logs/nohup-asa2-aa-$HOSTNAME.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False # isNotebook, False, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newssource.asattribution.utils import *\n",
    "from newssource.asattribution.asamin import *\n",
    "from newssource.asa.asapreproc import *\n",
    "from newssource.asa.models import *\n",
    "from newssource.metrics.ndcg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if not isNotebook:\n",
    "    matplotlib.use('Agg')\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning.kerasutils import *\n",
    "from machinelearning.kerasmodels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning import kerasutils\n",
    "from machinelearning.iterator import *\n",
    "from machinelearning.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepstyle.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if not isNotebook:\n",
    "    matplotlib.use('Agg')\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Cython ; git clone https://github.com/epfml/sent2vec.git ; cd ./sent2vec ; pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newssource.dbert.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.topicmodeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir(\"logs\") + \"/asa2-aa-\" + getHostname() + \".log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \\\n",
    "[\n",
    "    'uset0-l50-dpl50-d18-bc10',\n",
    "    'uset0-l50-dpl50-blogger.com',\n",
    "    'uset0-l50-dpl50-breitbart.com',\n",
    "    'uset0-l50-dpl50-businessinsider.com',\n",
    "    'uset0-l50-dpl50-cnn.com',\n",
    "    'uset0-l50-dpl50-guardian.co.uk',\n",
    "    'uset0-l50-dpl50-livejournal.com',\n",
    "    'uset0-l50-dpl50-nytimes.com',\n",
    "    'uset0-l50-dpl50-theguardian.com',\n",
    "    'uset0-l50-dpl50-washingtonpost.com',\n",
    "    'uset1-l50-dpl50-blogger.com',\n",
    "    'uset1-l50-dpl50-d18-bc10',\n",
    "    'uset1-l50-dpl50-livejournal.com',\n",
    "    'uset2-l50-dpl50-blogger.com',\n",
    "    'uset2-l50-dpl50-d18-bc10',\n",
    "    'uset2-l50-dpl50-livejournal.com',\n",
    "    'uset3-l50-dpl50-blogger.com',\n",
    "    'uset3-l50-dpl50-d18-bc10',\n",
    "    'uset3-l50-dpl50-livejournal.com',\n",
    "    'uset4-l50-dpl50-blogger.com',\n",
    "    'uset4-l50-dpl50-d18-bc10',\n",
    "    'uset4-l50-dpl50-livejournal.com',\n",
    "]\n",
    "files = [nosaveDir() + \"/Data/Asa2/detok-usets/\" + e + \"/0.ndjson.bz2\" for e in files]\n",
    "bp(files, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipisNumbers = \"85 86 87 92 95 59 56 58 57 84 83 82 81 88 63 03 93 07 06 90 60 62 61 01 02 89 94\".split()\n",
    "tipis = ['tipi' + e for e in sorted(tipisNumbers)]\n",
    "bp(tipis, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the uset for the current tipi:\n",
    "association = associate(tipis, files)\n",
    "bp(association, 5, logger)\n",
    "if not \"tipi\" in getHostname() or isDocker() or isHostname(\"titanv\"):\n",
    "    file = association['tipi07']\n",
    "else:\n",
    "    assert getHostname() in association\n",
    "    file = association[getHostname()]\n",
    "uset = file.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for results:\n",
    "dataCol = \"filtered_sentences\" # filtered_sentences, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters:\n",
    "randomTestsAmount = 0\n",
    "docLength = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging:\n",
    "log(getHostname() + \" handles \" + uset, logger)\n",
    "log(\"dataCol: \" + str(dataCol), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data:\n",
    "hashs = []\n",
    "docs = []\n",
    "labels = []\n",
    "flatDocs = []\n",
    "flatLowDocs = []\n",
    "flatTruncDocs = []\n",
    "flatTruncLowDocs = []\n",
    "detokDocs = []\n",
    "detokSentences = []\n",
    "for row in NDJson(file):\n",
    "    detokSentences.append(row['filtered_detokenized_sentences'])\n",
    "    detokDocs.append(row['filtered_detokenized'])\n",
    "    sentences = row[dataCol]\n",
    "    theHash = objectToHash(sentences)\n",
    "    hashs.append(theHash)\n",
    "    docs.append(sentences)\n",
    "    labels.append(row[\"label\"])\n",
    "    flattenedDoc = flattenLists(sentences)\n",
    "    flatDocs.append(flattenedDoc)\n",
    "    flatLowDocs.append([e.lower() for e in flattenedDoc])\n",
    "    truncatedDoc = flattenedDoc[:docLength]\n",
    "    flatTruncDocs.append(truncatedDoc)\n",
    "    flatTruncLowDocs.append([e.lower() for e in truncatedDoc])\n",
    "bp(docs, logger)\n",
    "tt.tic(\"Got documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(classes, indexLabels) = encodeMulticlassLabels(labels, encoding='index')\n",
    "bp(indexLabels, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(user, password, host) = getOctodsMongoAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCache = dict()\n",
    "def getAsaFeatures2\\\n",
    "(\n",
    "    docs,\n",
    "    flatDocs,\n",
    "    flatLowDocs,\n",
    "    flatTruncDocs,\n",
    "    flatTruncLowDocs,\n",
    "    detokDocs,\n",
    "    detokSentences,\n",
    "    \n",
    "    dataHash=None,\n",
    "    \n",
    "    useNMF=False,\n",
    "    nmfKwargs=None,\n",
    "    \n",
    "    useLDA=False,\n",
    "    ldaKwargs=None,\n",
    "    \n",
    "    useDbert=False,\n",
    "    dbertKwargs=None, # operation, layer, modelName\n",
    "    \n",
    "    useDeepStyle=False,\n",
    "    deepStyleKwargs=None,\n",
    "    deepStyleRoot=nosaveDir() + \"/asa2-train\",\n",
    "    \n",
    "    useTFIDF=False,\n",
    "    tfidfKwargs=None,\n",
    "    defaultTFIDFNIter=30,\n",
    "    \n",
    "    useDoc2Vec=False,\n",
    "    doc2VecKwargs=None,\n",
    "    d2vPath=nosaveDir() + \"/d2v/d2vmodel-t-ds22.02g-s300-w3-n15-e15-lTrue-adFalse-7bb8a\",\n",
    "    \n",
    "    useStylo=False,\n",
    "    styloKwargs=None,\n",
    "    \n",
    "    useUsent=False,\n",
    "    usentKwargs=None,\n",
    "    usentEmbeddingsPattern=nosaveDir() + \"/usent/usentEmbedding*.pickle\",\n",
    "    \n",
    "    useInferSent=False,\n",
    "    inferSentKwargs=None,\n",
    "    inferSentRoot=nosaveDir() + '/infersent',\n",
    "    \n",
    "    useBERT=False,\n",
    "    bertKwargs=None,\n",
    "    \n",
    "    useSent2Vec=False,\n",
    "    sent2VecKwargs=None,\n",
    "    sent2VecRoot=nosaveDir() + '/sent2vec',\n",
    "    defaultSent2VecModelName=\"wiki_unigrams.bin\",\n",
    "\n",
    "    logger=None,\n",
    "    verbose=True,\n",
    "):\n",
    "    global featuresCache\n",
    "    if dataHash is None:\n",
    "        logWarning(\"Please provide a data hash to prevent its computation each call\", logger)\n",
    "        dataHash = objectToHash([docs, flatDocs, flatLowDocs, flatTruncDocs, flatTruncLowDocs, detokDocs, detokSentences])\n",
    "    features = []\n",
    "    # NMF:\n",
    "    if useNMF:\n",
    "        h = objectToHash(['NMF', nmfKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            data = nmfFeatures(flatDocs, **nmfKwargs)\n",
    "            features.append(data)\n",
    "            featuresCache[h] = data\n",
    "    # LDA:\n",
    "    if useLDA:\n",
    "        h = objectToHash(['LDA', ldaKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            data = ldaFeatures(flatDocs, **ldaKwargs)\n",
    "            features.append(data)\n",
    "            featuresCache[h] = data\n",
    "    # DBert:\n",
    "    if useDbert:\n",
    "        h = objectToHash(['DBert', dbertKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            modelName = dbertKwargs['modelName']\n",
    "            if modelName is None:\n",
    "                modelPath = None\n",
    "            else:\n",
    "                if isDir(nosaveDir() + '/dbert-train/' + modelName):\n",
    "                    modelPath = sortedGlob(nosaveDir() + '/dbert-train/' + modelName + '/epochs/ep*')[-1]\n",
    "                else:\n",
    "                    modelPath = sortedGlob(nosaveDir() + '/dbert-tmp/' + modelName + '/epochs/ep*')[-1]\n",
    "            layer = dbertKwargs['layer']\n",
    "            # if modelName is None:\n",
    "            #     (user, password, host) = getOctodsMongoAuth()\n",
    "            #     dbertCache = SerializableDict(\"dbert-embeddings\",\n",
    "            #                                   user=user, host=host, password=password,\n",
    "            #                                   useMongodb=True, logger=logger)\n",
    "            # else:\n",
    "            dbertCache = SerializableDict(\"dbert-embeddings-\" + str(modelName),\n",
    "                          nosaveDir() + '/dbert-cache',\n",
    "                          useMongodb=False, logger=logger,\n",
    "                          loadRetry=30, loadSleepMin=0.5, loadSleepMax=30,\n",
    "                         readIsAnAction=False,)\n",
    "            embeddings = []\n",
    "            for doc in docs:\n",
    "                if modelPath is None:\n",
    "                    layer = 'distilbert'\n",
    "                (currentHash, cacheObject) = getDbertEmbeddingsHash(doc, layer, modelPath)\n",
    "                if modelPath is None:\n",
    "                    layer = None\n",
    "                assert currentHash in dbertCache\n",
    "                embeddings.append(dbertCache[currentHash]['embeddings'])\n",
    "            dbertData = []\n",
    "            if dbertKwargs['operation'] == \"first\":\n",
    "                for emb in embeddings:\n",
    "                    dbertData.append(emb[0])\n",
    "            elif dbertKwargs['operation'] == \"mean\":\n",
    "                for emb in embeddings:\n",
    "                    dbertData.append(np.mean(emb, axis=0))\n",
    "            dbertData = np.array(dbertData)\n",
    "            features.append(dbertData)\n",
    "            featuresCache[h] = dbertData\n",
    "    # DeepStyle:\n",
    "    if useDeepStyle:\n",
    "        h = objectToHash(['DeepStyle', deepStyleKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            m = DeepStyle(deepStyleRoot + \"/\" + deepStyleKwargs['modelPattern'])\n",
    "            embeddings = np.array([m.embed(doc) for doc in docs])\n",
    "            features.append(embeddings)\n",
    "            featuresCache[h] = embeddings\n",
    "    # TFIDF:\n",
    "    if useTFIDF:\n",
    "        if \"nIter\" not in tfidfKwargs:\n",
    "            tfidfKwargs[\"nIter\"] = defaultTFIDFNIter\n",
    "        h = objectToHash(['TFIDF', tfidfKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            data = flatTruncDocs if tfidfKwargs['truncate'] else docs\n",
    "            tfidfInstance = TFIDF(data, doLower=tfidfKwargs['doLower'], logger=logger, verbose=False)\n",
    "            tfidfData = tfidfInstance.getTFIDFMatrix()\n",
    "            svd = TruncatedSVD(n_components=tfidfKwargs['nComponents'],\n",
    "                               n_iter=tfidfKwargs['nIter'],\n",
    "                               random_state=42)\n",
    "            svdTFIDFData = svd.fit_transform(tfidfData)\n",
    "            features.append(svdTFIDFData)\n",
    "            featuresCache[h] = svdTFIDFData\n",
    "    # Doc2Vec:\n",
    "    if useDoc2Vec:\n",
    "        h = objectToHash(['Doc2Vec', doc2VecKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            data = flatTruncLowDocs if doc2VecKwargs['truncate'] else flatLowDocs\n",
    "            d2vModel = Doc2Vec.load(sortedGlob(d2vPath + \"/*model*.d2v\")[0])\n",
    "            d2vData = d2vTokenssToEmbeddings(data, d2vModel, logger=logger, verbose=False)\n",
    "            features.append(d2vData)\n",
    "            featuresCache[h] = d2vData\n",
    "    # Stylo:\n",
    "    if useStylo:\n",
    "        h = objectToHash(['Stylo', styloKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            styloVectors = []\n",
    "            for text in pb(detokDocs, logger=logger, message=\"Getting stylo features\", verbose=verbose):\n",
    "                styloVectors.append(stylo(text, asNpArray=True))\n",
    "            styloVectors = np.array(styloVectors)\n",
    "            features.append(styloVectors)\n",
    "            featuresCache[h] = styloVectors\n",
    "    # Usent:\n",
    "    if useUsent:\n",
    "        h = objectToHash(['Usent', usentKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            # We get all embeddgins from usent:\n",
    "            allHashes = set()\n",
    "            for doc in docs:\n",
    "                docHash = objectToHash(doc)\n",
    "                allHashes.add(docHash)\n",
    "                for sentence in doc:\n",
    "                    theHash = objectToHash(sentence)\n",
    "                    allHashes.add(theHash)\n",
    "            usentEmbeddings = dict()\n",
    "            for usentEmbeddingsFile in pb(sortedGlob(usentEmbeddingsPattern),\n",
    "                                          printRatio=0.1, logger=logger, message=\"Getting Usent embeddings from all files\"):\n",
    "                current = deserialize(usentEmbeddingsFile)\n",
    "                for theHash, value in current.items():\n",
    "                    if theHash in allHashes:\n",
    "                        usentEmbeddings[theHash] = value\n",
    "            assert len(allHashes) == len(usentEmbeddings)\n",
    "            if usentKwargs['operation'] == \"full\":\n",
    "                data = []\n",
    "                for doc in docs:\n",
    "                    theHash = objectToHash(doc)\n",
    "                    data.append(usentEmbeddings[theHash])\n",
    "                data = np.array(data)\n",
    "            elif usentKwargs['operation'] == \"mean\":\n",
    "                data = []\n",
    "                for doc in docs:\n",
    "                    docEmbeddings = []\n",
    "                    for sentence in doc:\n",
    "                        theHash = objectToHash(sentence)\n",
    "                        docEmbeddings.append(usentEmbeddings[theHash])\n",
    "                    docEmbedding = np.mean(docEmbeddings, axis=0)\n",
    "                    data.append(docEmbedding)\n",
    "                data = np.array(data)\n",
    "            features.append(data)\n",
    "            featuresCache[h] = data\n",
    "    # InferSent:\n",
    "    if useInferSent:\n",
    "        h = objectToHash(['InferSent', inferSentKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            V = inferSentKwargs['V']\n",
    "            operation = inferSentKwargs['operation']\n",
    "            MODEL_PATH = inferSentRoot + '/infersent%s.pkl' % V\n",
    "            params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                            'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "            infersent = InferSent(params_model)\n",
    "            infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "            if V == 2:\n",
    "                W2V_PATH = inferSentRoot + '/fastText/crawl-300d-2M.vec'\n",
    "            else:\n",
    "                W2V_PATH = inferSentRoot + '/GloVe/glove.840B.300d.txt'\n",
    "            infersent.set_w2v_path(W2V_PATH)\n",
    "            infersent.build_vocab(detokDocs, tokenize=True)\n",
    "            if operation == \"full\":\n",
    "                detokDocsForInferSent = []\n",
    "                for current in detokDocs:\n",
    "                    detokDocsForInferSent.append(current[:10000])\n",
    "                isData = infersent.encode(detokDocsForInferSent, tokenize=True)\n",
    "            elif operation == \"mean\":\n",
    "                isData = []\n",
    "                for currentDetokSentences in detokSentences:\n",
    "                    embedding = np.mean(infersent.encode(currentDetokSentences, tokenize=True), axis=0)\n",
    "                    isData.append(embedding)\n",
    "                isData = np.array(isData)\n",
    "            features.append(isData)\n",
    "            featuresCache[h] = isData\n",
    "    # BERT:\n",
    "    if useBERT:\n",
    "        h = objectToHash(['BERT', bertKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            (user, password, host) = getOctodsMongoAuth()\n",
    "            bertCache = SerializableDict(\"newsid-bretcache\",\n",
    "                                         useMongodb=True,\n",
    "                                         user=user, password=password, host=host,\n",
    "                                         logger=logger)\n",
    "            bertData = []\n",
    "            for doc in docs:\n",
    "                theHash = objectToHash(doc)\n",
    "                current = bertCache[theHash]\n",
    "                bertData.append(current)\n",
    "            bertData = np.array(bertData)\n",
    "            features.append(bertData)\n",
    "            featuresCache[h] = bertData\n",
    "    # Sent2Vec:\n",
    "    if useSent2Vec:\n",
    "        if sent2VecKwargs is None:\n",
    "            sent2VecKwargs = dict()\n",
    "        if 'modelName' not in sent2VecKwargs:\n",
    "            sent2VecKwargs['modelName'] = defaultSent2VecModelName\n",
    "        h = objectToHash(['Sent2Vec', sent2VecKwargs, dataHash])\n",
    "        if h in featuresCache:\n",
    "            features.append(featuresCache[h])\n",
    "        else:\n",
    "            modelName = sent2VecKwargs['modelName']\n",
    "            s2vModel = sent2vec.Sent2vecModel()\n",
    "            s2vModel.load_model(sent2VecRoot + '/' + modelName)\n",
    "            operation = sent2VecKwargs['operation']\n",
    "            if operation == \"full\":\n",
    "                s2vData = s2vModel.embed_sentences(detokDocs)\n",
    "            elif operation == \"mean\":\n",
    "                s2vData = []\n",
    "                for currentDetokSentences in detokSentences:\n",
    "                    embedding = np.mean(s2vModel.embed_sentences(currentDetokSentences), axis=0)\n",
    "                    s2vData.append(embedding)\n",
    "                s2vData = np.array(s2vData)\n",
    "            try:\n",
    "                s2vModel.release_shared_mem(sent2VecRoot + '/' + modelName)\n",
    "                s2vModel = None\n",
    "            except Exception as e:\n",
    "                logException(e, logger)\n",
    "            features.append(s2vData)\n",
    "            featuresCache[h] = s2vData\n",
    "    # Concatenation:\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoubleCombinasons():\n",
    "    for i in range(len(uniqueUses)):\n",
    "        for u in range(i + 1, len(uniqueUses)):\n",
    "            yield (uniqueUses[i], uniqueUses[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(features, labels, validationRatio=0.3, doFigShow=isNotebook, logger=None, verbose=True):\n",
    "    clf = linear_model.SGDClassifier()\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(features, labels, test_size=validationRatio, random_state=42)\n",
    "    scores = scikitLearnFit(clf, xTrain, yTrain, xTest, yTest, doFigShow=doFigShow, doFigSave=False, logger=logger)\n",
    "    log(\"Features shape: \" + str(features.shape), logger=logger, verbose=verbose)\n",
    "    return max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueUses = ['useNMF', 'useLDA', 'useDbert', 'useTFIDF', 'useStylo', 'useDeepStyle', 'useBERT', 'useDoc2Vec', 'useUsent', 'useInferSent', 'useSent2Vec']\n",
    "allKwargs = \\\n",
    "{\n",
    "    'nmfKwargs': {},\n",
    "    'ldaKwargs': {},\n",
    "    # 'dbertKwargs': {'operation': 'mean', 'layer': 'distilbert', 'modelName': '94bef_ep32'},\n",
    "    'dbertKwargs': {'operation': 'mean', 'layer': None, 'modelName': None},\n",
    "    'deepStyleKwargs': {'modelPattern': '6ebdd3e05d4388c658ca2d5c53b0bc36'},\n",
    "    'tfidfKwargs': {'truncate': False, 'doLower': True, 'nComponents': 50},\n",
    "    'doc2VecKwargs': {'truncate': False},\n",
    "    'styloKwargs': None,\n",
    "    'usentKwargs': {'operation': 'mean'},\n",
    "    'inferSentKwargs': {'V': 1, 'operation': 'mean'},\n",
    "    'bertKwargs': None,\n",
    "    'sent2VecKwargs': {'operation': 'mean'},\n",
    "}\n",
    "useToKwargsMap = \\\n",
    "{\n",
    "    'useNMF': 'nmfKwargs',\n",
    "    'useLDA': 'ldaKwargs',\n",
    "    'useDbert': 'dbertKwargs',\n",
    "    'useDeepStyle': 'deepStyleKwargs',\n",
    "    'useTFIDF': 'tfidfKwargs',\n",
    "    'useDoc2Vec': 'doc2VecKwargs',\n",
    "    'useStylo': 'styloKwargs',\n",
    "    'useUsent': 'usentKwargs',\n",
    "    'useInferSent': 'inferSentKwargs',\n",
    "    'useBERT': 'bertKwargs',\n",
    "    'useSent2Vec': 'sent2VecKwargs',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinasons = [(e,) for e in uniqueUses] + list(getDoubleCombinasons())\n",
    "bp(combinasons, 5, logger)\n",
    "log(\"Count of combinasons: \" + str(len(combinasons)), logger)\n",
    "log(\"Count of points: \" + str(len(combinasons) * len(files)), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    logWarning(\"Removing combs that do not have useDbert\", logger)\n",
    "    newCombinasons = []\n",
    "    for current in combinasons:\n",
    "        # if 'useDbert' in current and ('useNMF' in current or 'useLDA' in current):\n",
    "        # if 'useNMF' in current or 'useLDA' in current:\n",
    "        if 'useDbert' in current:\n",
    "        # if 'useDbert' in current and 'useNMF' in current:\n",
    "            newCombinasons.append(current)\n",
    "    combinasons = newCombinasons\n",
    "    bp(combinasons, 3, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    combinasons = combinasons[:1]\n",
    "    bp(combinasons, 3, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentsArgs = \\\n",
    "(\n",
    "    docs,\n",
    "    flatDocs,\n",
    "    flatLowDocs,\n",
    "    flatTruncDocs,\n",
    "    flatTruncLowDocs,\n",
    "    detokDocs,\n",
    "    detokSentences,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHash = objectToHash(documentsArgs)\n",
    "bp('dataHash: ' + dataHash, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA:\n",
    "if True:\n",
    "    results = SerializableDict('asa2-aa', useMongodb=True, user=user, password=password, host=host, logger=logger)\n",
    "    for comb in pb(combinasons, logger=logger):\n",
    "        currentKwargs = copy.deepcopy(allKwargs)\n",
    "        for unique in comb:\n",
    "            currentKwargs[unique] = True\n",
    "        result = copy.deepcopy(currentKwargs)\n",
    "        for key in useToKwargsMap:\n",
    "            if not (dictContains(result, key) and result[key]):\n",
    "                del result[useToKwargsMap[key]]\n",
    "        result['uset'] = uset\n",
    "        result['dataCol'] = dataCol\n",
    "        theHash = objectToHash(result)\n",
    "        if theHash not in results:\n",
    "            features = getAsaFeatures2\\\n",
    "            (\n",
    "                *documentsArgs,\n",
    "                logger=logger,\n",
    "                verbose=True,\n",
    "                dataHash=dataHash,\n",
    "                **currentKwargs,\n",
    "            )\n",
    "            score = getScore(features, indexLabels, logger=logger, doFigShow=False)\n",
    "            log(\"Score of \" + str(comb) + \": \" + str(truncateFloat(score, 3)), logger)\n",
    "            result['score'] = score\n",
    "            results[theHash] = result\n",
    "        else:\n",
    "            log(\"Found the score of \" + str(comb) + \": \" + str(truncateFloat(results[theHash]['score'], 3)), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering:\n",
    "if False:\n",
    "    results = SerializableDict('asa2-clustering-comb', useMongodb=True, user=user, password=password, host=host, logger=logger)\n",
    "    for comb in pb(combinasons, logger=logger):\n",
    "        currentKwargs = copy.deepcopy(allKwargs)\n",
    "        for unique in comb:\n",
    "            currentKwargs[unique] = True\n",
    "        result = copy.deepcopy(currentKwargs)\n",
    "        for key in useToKwargsMap:\n",
    "            if not (dictContains(result, key) and result[key]):\n",
    "                del result[useToKwargsMap[key]]\n",
    "        result['uset'] = uset\n",
    "        result['dataCol'] = dataCol\n",
    "        theHash = objectToHash(result)\n",
    "        if theHash not in results:\n",
    "            features = getAsaFeatures2\\\n",
    "            (\n",
    "                *documentsArgs,\n",
    "                logger=logger,\n",
    "                verbose=True,\n",
    "                dataHash=dataHash,\n",
    "                **currentKwargs,\n",
    "            )\n",
    "            # We compute and store score:\n",
    "            data = features\n",
    "            simMatrix = pairwiseCosineSimilarity(data)\n",
    "            score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "            log(\"SimRank of \" + str(comb) + \": \" + str(truncateFloat(score, 3)), logger)\n",
    "            # calharScore = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "            # log(\"CalHar: \" + str(calharScore), logger)\n",
    "            # davbScore = metrics.davies_bouldin_score(data, indexLabels)\n",
    "            # log(\"DavB: \" + str(davbScore), logger)\n",
    "            # Adding results:\n",
    "            result['score'] = score\n",
    "            results[theHash] = result\n",
    "        else:\n",
    "            log(\"Found the score of \" + str(comb) + \": \" + str(truncateFloat(results[theHash]['score'], 3)), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    allKwargs['dbertKwargs'] = {'operation': 'mean', 'layer': None, 'modelName': None}\n",
    "    currentKwargs = copy.deepcopy(allKwargs)\n",
    "    currentKwargs['useDbert'] = True\n",
    "    dbertBaseFeatures = getAsaFeatures2\\\n",
    "    (\n",
    "        *documentsArgs,\n",
    "        logger=logger,\n",
    "        verbose=True,\n",
    "        dataHash=dataHash,\n",
    "        **currentKwargs,\n",
    "    )\n",
    "    bp(dbertBaseFeatures, logger)\n",
    "    allKwargs['dbertKwargs'] = {'operation': 'mean', 'layer': 'distilbert', 'modelName': '94bef_ep32'}\n",
    "    currentKwargs = copy.deepcopy(allKwargs)\n",
    "    currentKwargs['useDbert'] = True\n",
    "    dbertEp32Features = getAsaFeatures2\\\n",
    "    (\n",
    "        *documentsArgs,\n",
    "        logger=logger,\n",
    "        verbose=True,\n",
    "        dataHash=dataHash,\n",
    "        **currentKwargs,\n",
    "    )\n",
    "    bp(dbertEp32Features, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    features = np.concatenate([dbertEp32Features, dbertBaseFeatures], axis=1)\n",
    "    bp(dbertEp32Features, logger)\n",
    "    log(features.shape, logger)\n",
    "    score = getScore(features, indexLabels, logger=logger, doFigShow=False)\n",
    "    log(\"Score of bdert base and 94bef_ep32 for \" + uset + \" --> \" + str(score), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    features = np.concatenate([dbertEp32Features, dbertBaseFeatures], axis=1)\n",
    "    data = features\n",
    "    simMatrix = pairwiseCosineSimilarity(data)\n",
    "    score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "    log(\"simrank_uuu Score of bdert base and 94bef_ep32 for \" + uset + \" --> \" + str(score), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isNotebook:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = SerializableDict('asa2-aa', useMongodb=True, user=user, password=password, host=host, logger=logger)\n",
    "# results = SerializableDict('asa2-clustering-comb', useMongodb=True, user=user, password=password, host=host, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(o):\n",
    "    k = set(o.keys()) if isinstance(o, dict) else set(o)\n",
    "    return [k for k in o if re.search(\"^use[A-Z].*$\", k)]\n",
    "def featuresCount(*args, **kwargs):\n",
    "    return len(extractFeatures(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(fCount=None, usetPattern=None):\n",
    "    if usetPattern is None:\n",
    "        usetPattern = '.*'\n",
    "    if \".*\" not in usetPattern:\n",
    "        usetPattern = \".*\" + usetPattern + \".*\"\n",
    "    if '^' not in usetPattern:\n",
    "        usetPattern = '^' + usetPattern\n",
    "    if '$' not in usetPattern:\n",
    "        usetPattern = usetPattern + '$'\n",
    "    r = []\n",
    "    for _, e in results.items():\n",
    "        if re.match(usetPattern, e['uset']) and (fCount is None or featuresCount(e) == fCount):\n",
    "            r.append(e)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = getData(1, uset)\n",
    "# r = getData(1, 'd18-bc10')\n",
    "# r = getData(1, '.*')\n",
    "# r = getData(1, 'blogger')\n",
    "# r = getData(1, 'livejournal')\n",
    "r = getData(1, 'washington')\n",
    "# r = getData(1, 'breitbart')\n",
    "# r = getData(1, 'business')\n",
    "# r = getData(1, 'cnn')\n",
    "# r = getData(1, 'guardian.co.uk')\n",
    "# r = getData(1, 'theguardian.com')\n",
    "# r = getData(1, 'nytimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDbert(r, dbertKwargsFilter, logger=None, verbose=True):\n",
    "    logWarning(\"Filtering DBert with \" + str(dbertKwargsFilter), logger)\n",
    "    deletedCount = 0\n",
    "    acceptedCount = 0\n",
    "    newR = []\n",
    "    for current in r:\n",
    "        if dictContains(current, 'useDbert') and current['useDbert']:\n",
    "            ok = True\n",
    "            for key in dbertKwargsFilter:\n",
    "                if current['dbertKwargs'][key] != dbertKwargsFilter[key]:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                newR.append(current)\n",
    "                acceptedCount += 1\n",
    "            else:\n",
    "                deletedCount += 1\n",
    "        else:\n",
    "            newR.append(current)\n",
    "    log(\"deletedCount: \" + str(deletedCount), logger)\n",
    "    log(\"acceptedCount: \" + str(acceptedCount), logger)\n",
    "    return newR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = filterDbert(r, {'operation': 'mean', 'layer': None, 'modelName': None}, logger=logger)\n",
    "r = filterDbert(r, {'operation': 'mean', 'layer': 'distilbert', 'modelName': '94bef_ep32'}, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bp(r, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Amount of usets: \" + str(len(set([e['uset'] for e in r]))))\n",
    "log(\"Amount of models: \" + str(len(set([str(extractFeatures(e)) for e in r]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current in r:\n",
    "    f = extractFeatures(current)[0]\n",
    "    if f not in models:\n",
    "        models[f] = []\n",
    "    models[f].append(current['score'])\n",
    "bp(models, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in models.items():\n",
    "    print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models.keys():\n",
    "    models[key] = float(np.mean(models[key]))\n",
    "for m, s in sortBy(models.items(), 1): print(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = SerializableDict('asa2-aa', useMongodb=True, user=user, password=password, host=host, logger=logger)\n",
    "results = SerializableDict('asa2-clustering-comb', useMongodb=True, user=user, password=password, host=host, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = getData(2, '.*')\n",
    "# r = getData(2, 'd18-bc10')\n",
    "# r = getData(2, 'blogger')\n",
    "# r = getData(2, 'livejournal')\n",
    "# r = getData(2, 'washington')\n",
    "# r = getData(2, 'breitbart')\n",
    "# r = getData(2, 'business')\n",
    "# r = getData(2, 'cnn')\n",
    "# r = getData(2, 'guardian.co.uk')\n",
    "# r = getData(2, 'theguardian.com')\n",
    "# r = getData(2, 'nytimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = filterDbert(r, {'operation': 'mean', 'layer': None, 'modelName': None}, logger=logger)\n",
    "# r = filterDbert(r, {'operation': 'mean', 'layer': 'distilbert', 'modelName': '94bef_ep32'}, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Amount of usets: \" + str(len(set([e['uset'] for e in r]))))\n",
    "log(\"Amount of models: \" + str(len(set([str(extractFeatures(e)) for e in r]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current in r:\n",
    "    f = str(extractFeatures(current))\n",
    "    if f not in models:\n",
    "        models[f] = []\n",
    "    models[f].append(current['score'])\n",
    "bp(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in sorted(models.keys()):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in models.items():\n",
    "    print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models.keys():\n",
    "    models[key] = float(np.mean(models[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, s in sortBy(models.items(), 1):\n",
    "    print(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
