{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ~/asa2-clustering-logs ; jupython --no-tail --venv st-venv -o nohup-asa2-clustering-$HOSTNAME.out ~/notebooks/asa/eval/asa2-clustering.ipynb\n",
    "# observe ~/asa2-clustering-logs/nohup-asa2-clustering-$HOSTNAME.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False # isNotebook, False, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newssource.asattribution.utils import *\n",
    "from newssource.asattribution.asamin import *\n",
    "from newssource.asa.asapreproc import *\n",
    "from newssource.asa.models import *\n",
    "from newssource.metrics.ndcg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if not isNotebook:\n",
    "    matplotlib.use('Agg')\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning.kerasutils import *\n",
    "from machinelearning.kerasmodels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning import kerasutils\n",
    "from machinelearning.iterator import *\n",
    "from machinelearning.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepstyle.model import *\n",
    "from machinelearning import tf2utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newssource.dbert.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.topicmodeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir(\"logs\") + \"/asa2-clustering-\" + getHostname() + \".log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \\\n",
    "[\n",
    "    'uset0-l50-dpl50-d18-bc10',\n",
    "    'uset0-l50-dpl50-blogger.com',\n",
    "    'uset0-l50-dpl50-breitbart.com',\n",
    "    'uset0-l50-dpl50-businessinsider.com',\n",
    "    'uset0-l50-dpl50-cnn.com',\n",
    "    'uset0-l50-dpl50-guardian.co.uk',\n",
    "    'uset0-l50-dpl50-livejournal.com',\n",
    "    'uset0-l50-dpl50-nytimes.com',\n",
    "    'uset0-l50-dpl50-theguardian.com',\n",
    "    'uset0-l50-dpl50-washingtonpost.com',\n",
    "    'uset1-l50-dpl50-blogger.com',\n",
    "    'uset1-l50-dpl50-d18-bc10',\n",
    "    'uset1-l50-dpl50-livejournal.com',\n",
    "    'uset2-l50-dpl50-blogger.com',\n",
    "    'uset2-l50-dpl50-d18-bc10',\n",
    "    'uset2-l50-dpl50-livejournal.com',\n",
    "    'uset3-l50-dpl50-blogger.com',\n",
    "    'uset3-l50-dpl50-d18-bc10',\n",
    "    'uset3-l50-dpl50-livejournal.com',\n",
    "    'uset4-l50-dpl50-blogger.com',\n",
    "    'uset4-l50-dpl50-d18-bc10',\n",
    "    'uset4-l50-dpl50-livejournal.com',\n",
    "]\n",
    "files = [nosaveDir() + \"/Data/Asa2/detok-usets/\" + e + \"/0.ndjson.bz2\" for e in files]\n",
    "bp(files, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipisNumbers = \"92 95 90 05 87 01 81 84 85 82 93 62 58 88 57 03 56 86 59 07 06 83 63 61 02 60 94\".split()\n",
    "tipis = ['tipi' + e for e in sorted(tipisNumbers)]\n",
    "bp(tipis, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the uset for the current tipi:\n",
    "association = associate(tipis, files[:len(tipis)])\n",
    "bp(association, 5, logger)\n",
    "if not \"tipi\" in getHostname() or isDocker() or isHostname(\"titanv\"):\n",
    "    file = association['tipi59']\n",
    "else:\n",
    "    if getHostname() not in association:\n",
    "        log(\"Exiting because \" + getHostname() + \" is not in associations\", logger)\n",
    "        exit()\n",
    "    file = association[getHostname()]\n",
    "uset = file.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for results:\n",
    "dataCol = \"filtered_sentences\" # filtered_sentences, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters:\n",
    "randomTestsAmount = 0\n",
    "docLength = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging:\n",
    "log(getHostname() + \" handles \" + uset, logger)\n",
    "log(\"dataCol: \" + str(dataCol), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data:\n",
    "hashs = []\n",
    "docs = []\n",
    "labels = []\n",
    "flattenedDocs = []\n",
    "flattenedAndLoweredDocs = []\n",
    "truncatedDocs = []\n",
    "truncatedAndLoweredDocs = []\n",
    "detokSentences = []\n",
    "detokDocs = []\n",
    "for row in NDJson(file):\n",
    "    detokSentences.append(row['filtered_detokenized_sentences'])\n",
    "    detokDocs.append(row['filtered_detokenized'])\n",
    "    sentences = row[dataCol]\n",
    "    theHash = objectToHash(sentences)\n",
    "    hashs.append(theHash)\n",
    "    docs.append(sentences)\n",
    "    labels.append(row[\"label\"])\n",
    "    flattenedDoc = flattenLists(sentences)\n",
    "    flattenedDocs.append(flattenedDoc)\n",
    "    flattenedAndLoweredDocs.append([e.lower() for e in flattenedDoc])\n",
    "    truncatedDoc = flattenedDoc[:docLength]\n",
    "    truncatedDocs.append(truncatedDoc)\n",
    "    truncatedAndLoweredDocs.append([e.lower() for e in truncatedDoc])\n",
    "bp(docs, logger)\n",
    "tt.tic(\"Got documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    bp(docs[0])\n",
    "    bp(detokDocs[0])\n",
    "    bp(detokSentences[0])\n",
    "    bp(labels[0])\n",
    "    bp(flattenedDocs[0])\n",
    "    bp(flattenedAndLoweredDocs[0])\n",
    "    bp(truncatedDocs[0])\n",
    "    bp(truncatedAndLoweredDocs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(classes, indexLabels) = encodeMulticlassLabels(labels, encoding='index')\n",
    "bp(indexLabels, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(user, password, host) = getOctodsMongoAuth()\n",
    "results = SerializableDict('asa2-clustering', useMongodb=True, user=user, password=password, host=host, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultBase(model, metric, verbose=True, **kwargs):\n",
    "    global logger\n",
    "    global uset\n",
    "    global dataCol\n",
    "    global results\n",
    "    currentResult = kwargs\n",
    "    currentResult['model'] = model\n",
    "    currentResult['metric'] = metric\n",
    "    currentResult['uset'] = uset\n",
    "    currentResult['dataCol'] = dataCol\n",
    "    currentHash = objectToHash(currentResult)\n",
    "    return (currentHash, currentResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addResult(model, metric, score, verbose=True, **kwargs):\n",
    "    global logger\n",
    "    global uset\n",
    "    global dataCol\n",
    "    global results\n",
    "    (currentHash, currentResult) = getResultBase(model, metric, verbose=verbose, **kwargs)\n",
    "    currentResult['score'] = score\n",
    "    results[currentHash] = currentResult\n",
    "    bp(currentResult, logger, 5, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    allResults = [e[1] for e in results.items()]\n",
    "    # bp(allResults, logger)\n",
    "    r = dict()\n",
    "    for current in allResults:\n",
    "        # dataCol = current['dataCol']\n",
    "        uset = current['uset']\n",
    "        model = current['model']\n",
    "        metric = current['metric']\n",
    "        score = current['score']\n",
    "        if uset not in r: r[uset] = dict()\n",
    "        if model not in r[uset]: r[uset][model] = dict()\n",
    "        assert metric not in r[uset][model]\n",
    "        r[uset][model][metric] = score\n",
    "    bp(r, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    print(\"Models:\")\n",
    "    bp(set(r[\"uset0-l50-dpl50-d18-bc10\"].keys()), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    ######## Uset ########\n",
    "    # usetPattern = \".*\"\n",
    "    # usetPattern = uset\n",
    "    # usetPattern = \"uset0-l50-dpl50-d18-bc10\"\n",
    "    # usetPattern = \".*d18-bc10\"\n",
    "    usetPattern = \".*blogger.*\"\n",
    "    # usetPattern = \".*livejournal.*\"\n",
    "    # usetPattern = \".*washingtonpost.*\"\n",
    "    # usetPattern = \".*breitbart.*\"\n",
    "    # usetPattern = \".*businessinsider.*\"\n",
    "    # usetPattern = \".*cnn.*\"\n",
    "    # usetPattern = \".*guardian.*co.*uk.*\"\n",
    "    # usetPattern = \".*theguardian.*\"\n",
    "    # usetPattern = \".*nytimes.*\"\n",
    "    # usetPattern = \".*c50.*\"\n",
    "    ######## Model ########\n",
    "    # modelPattern = \".*tfidf$\"\n",
    "    # modelPattern = \".*nmf.*\"\n",
    "    # modelPattern = \".*lda.*\"\n",
    "    # modelPattern = \".*tfidf.*50$\"\n",
    "    # modelPattern = \".*dbert.*-distilbert-mean\"\n",
    "    # modelPattern = \".*dbert.*\"\n",
    "    modelPattern = \".*dbert.*ep32.*-distilbert-mean\"\n",
    "    # modelPattern = \"sna\"\n",
    "    # modelPattern = \"lstm\"\n",
    "    # modelPattern = \"bert\"\n",
    "    # modelPattern = \"stylo\"\n",
    "    # modelPattern = \".*usent.*mean.*\"\n",
    "    # modelPattern = \".*usent.*full.*\"\n",
    "    # modelPattern = \".*infersent-1.*mean.*\"\n",
    "    # modelPattern = \".*infersent-1.*full.*\"\n",
    "    # modelPattern = \".*word2vec.*\"\n",
    "    # modelPattern = \".*doc2vec.*\"\n",
    "    # modelPattern = \".*sent2vec.*mean.*\"\n",
    "    # modelPattern = \".*sent2vec.*full.*\"\n",
    "    # modelPattern = \".*random.*\"\n",
    "    ######## Metric ########\n",
    "    metricPattern = \".*\"\n",
    "    # metricPattern = \"simrank\"\n",
    "    # metricPattern = \"calhar\"\n",
    "    # metricPattern = \"davb\"\n",
    "    ######## Filtering ########\n",
    "    fr = dict()\n",
    "    for uset, models in r.items():\n",
    "        if re.match(usetPattern, uset):\n",
    "            for model, metrs in models.items():\n",
    "                if re.match(modelPattern, model):\n",
    "                    for metric, score in metrs.items():\n",
    "                        if re.match(metricPattern, metric):\n",
    "                            if uset not in fr: fr[uset] = dict()\n",
    "                            if model not in fr[uset]: fr[uset][model] = dict()\n",
    "                            fr[uset][model][metric] = score\n",
    "    ######## Print ########\n",
    "    bp(fr, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    models = set(flattenLists([list(v.keys()) for k, v in fr.items()]))\n",
    "    scores = dict()\n",
    "    for model in models:\n",
    "        scores[model] = dict()\n",
    "    for uset, models in fr.items():\n",
    "        for model, metrs in models.items():\n",
    "            for metric, score in metrs.items():\n",
    "                if metric not in scores[model]: scores[model][metric] = []\n",
    "                scores[model][metric].append(score)\n",
    "    log(\"For these usets: \" + str(set(fr.keys())), logger)\n",
    "    for model, metrs in scores.items():\n",
    "        for metric, currentScores in metrs.items():\n",
    "            token = model + \" (\" + str(len(currentScores)) + \")\" + \" ==> \" + metric\n",
    "            log(token, logger)\n",
    "            meanScore = sum(currentScores) / len(currentScores)\n",
    "            meanScore = truncateFloat(meanScore, 3)\n",
    "            log(meanScore, logger)\n",
    "    # bp(scores, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire line:\n",
    "if isNotebook:\n",
    "    ######## Uset ########\n",
    "    # usetPattern = \".*\"\n",
    "    # usetPattern = \".*d18-bc10\"\n",
    "    # usetPattern = \".*blogger.*\"\n",
    "    usetPatterns = [\".*livejournal.*\", \".*washingtonpost.*\", \".*breitbart.*\", \".*businessinsider.*\", \".*cnn.*\", \".*guardian.*co.*uk.*\", \".*theguardian.*\", \".*nytimes.*\"]\n",
    "    text = \"\"\n",
    "    for usetPattern in usetPatterns:\n",
    "        ######## Model ########\n",
    "        # modelPattern = \".*tfidf$\"\n",
    "        # modelPattern = \".*tfidf.*50$\"\n",
    "        # modelPattern = \".*dbert.*None.*-mean\"\n",
    "        modelPattern = \".*nmf.*\"\n",
    "        # modelPattern = \".*lda.*\"\n",
    "        # modelPattern = \"sna\"\n",
    "        # modelPattern = \"lstm\"\n",
    "        # modelPattern = \"bert\"\n",
    "        # modelPattern = \"stylo\"\n",
    "        # modelPattern = \".*usent.*mean.*\"\n",
    "        # modelPattern = \".*usent.*full.*\"\n",
    "        # modelPattern = \".*infersent-1.*mean.*\"\n",
    "        # modelPattern = \".*infersent-1.*full.*\"\n",
    "        # modelPattern = \".*word2vec.*\"\n",
    "        # modelPattern = \".*doc2vec.*\"\n",
    "        # modelPattern = \".*sent2vec.*mean.*\"\n",
    "        # modelPattern = \".*sent2vec.*full.*\"\n",
    "        # modelPattern = \".*random.*\"\n",
    "        ######## Metric ########\n",
    "        # metricPattern = \".*\"\n",
    "        metricPattern = \"simrank\"\n",
    "        # metricPattern = \"calhar\"\n",
    "        # metricPattern = \"davb\"\n",
    "        ######## Filtering ########\n",
    "        fr = dict()\n",
    "        for uset, models in r.items():\n",
    "            if re.match(usetPattern, uset):\n",
    "                for model, metrs in models.items():\n",
    "                    if re.match(modelPattern, model):\n",
    "                        for metric, score in metrs.items():\n",
    "                            if re.match(metricPattern, metric):\n",
    "                                if uset not in fr: fr[uset] = dict()\n",
    "                                if model not in fr[uset]: fr[uset][model] = dict()\n",
    "                                fr[uset][model][metric] = score\n",
    "        ######## Print ########\n",
    "        models = set(flattenLists([list(v.keys()) for k, v in fr.items()]))\n",
    "        scores = dict()\n",
    "        for model in models:\n",
    "            scores[model] = dict()\n",
    "        for uset, models in fr.items():\n",
    "            for model, metrs in models.items():\n",
    "                for metric, score in metrs.items():\n",
    "                    if metric not in scores[model]: scores[model][metric] = []\n",
    "                    scores[model][metric].append(score)\n",
    "        for model, metrs in scores.items():\n",
    "            for metric, currentScores in metrs.items():\n",
    "                token = model + \" (\" + str(len(currentScores)) + \")\" + \" ==> \" + metric\n",
    "                meanScore = sum(currentScores) / len(currentScores)\n",
    "                meanScore = truncateFloat(meanScore, 3)\n",
    "                text += str(meanScore) + \"\\t\"\n",
    "    log(text, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model is dbert-94bef_ep32-distilbert-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "data = ldaFeatures(flattenedDocs, logger=logger)\n",
    "model = 'lda'\n",
    "meta = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute and store score:\n",
    "simMatrix = pairwiseCosineSimilarity(data)\n",
    "score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "addResult(model, 'simrank', score, **meta)\n",
    "score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "addResult(model, 'calhar', score, **meta)\n",
    "score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"LDA done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "data = nmfFeatures(flattenedDocs, logger=logger)\n",
    "model = 'nmf'\n",
    "meta = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute and store score:\n",
    "simMatrix = pairwiseCosineSimilarity(data)\n",
    "score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "addResult(model, 'simrank', score, **meta)\n",
    "score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "addResult(model, 'calhar', score, **meta)\n",
    "score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"NMF done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['94bef_ep32']\n",
    "log(\"DBert models: \" + str(models), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = []\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        modelsPath.append(None)\n",
    "    else:\n",
    "        if isDir(nosaveDir() + '/dbert-train/' + model):\n",
    "            modelsPath.append(sortedGlob(nosaveDir() + '/dbert-train/' + model + '/epochs/ep*')[-1])\n",
    "        else:\n",
    "            modelsPath.append(sortedGlob(nosaveDir() + '/dbert-tmp/' + model + '/epochs/ep*')[-1])\n",
    "bp(modelsPath, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLength = 512\n",
    "layers = ['distilbert']\n",
    "operations = ['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(len(models) * len(operations) * len(layers), logger=logger, printRatio=0.001, message='Computing DBert scores')\n",
    "for i in range(len(models)):\n",
    "    modelName, modelPath, epoch = None, None, None\n",
    "    if models[i] is not None:\n",
    "        modelName = str(models[i])\n",
    "        modelPath = modelsPath[i]\n",
    "        epoch = getAllNumbers(modelPath)[-1]\n",
    "    dbertCache = SerializableDict(\"dbert-embeddings-\" + str(modelName),\n",
    "                                  nosaveDir() + '/dbert-cache',\n",
    "                                  useMongodb=False, logger=logger,\n",
    "                                  loadRetry=30, loadSleepMin=0.5, loadSleepMax=30,\n",
    "                                 readIsAnAction=False,)\n",
    "    assert len(dbertCache) > 0\n",
    "    log(\"Starting to compute DBert scores for \" + str(modelName), logger)\n",
    "    for layer in layers:\n",
    "        embeddings = []\n",
    "        for doc in docs:\n",
    "            (currentHash, cacheObject) = getDbertEmbeddingsHash(doc, layer, modelPath, maxLength)\n",
    "            assert currentHash in dbertCache\n",
    "            embeddings.append(dbertCache[currentHash]['embeddings'])\n",
    "        for operation in operations:\n",
    "            dbertData = []\n",
    "            if operation == 'first':\n",
    "                for emb in embeddings:\n",
    "                    dbertData.append(emb[0])\n",
    "            elif operation == 'mean':\n",
    "                for emb in embeddings:\n",
    "                    dbertData.append(np.mean(emb, axis=0))\n",
    "            # We set vars:\n",
    "            data = np.array(dbertData)\n",
    "            model = 'dbert-' + str(modelName) + '-' + layer + '-' + operation\n",
    "            meta = {'modelName': modelName, 'layer': layer, 'operation': operation, 'epoch': epoch}\n",
    "            # We compute and store score:\n",
    "            simMatrix = pairwiseCosineSimilarity(data)\n",
    "            score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "            addResult(model, 'simrank', score, **meta)\n",
    "            score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "            addResult(model, 'calhar', score, **meta)\n",
    "            score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "            addResult(model, 'davb', score, **meta)\n",
    "            pbar.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"DBert_done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [10, 100]\n",
    "rdAmount = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdReprs = []\n",
    "for dim in dimensions:\n",
    "    for i in range(rdAmount):\n",
    "        rdReprs.append(np.random.rand(len(docs), dim))\n",
    "bp(rdReprs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simRankScores = []\n",
    "calharScores = []\n",
    "davbScores = []\n",
    "for current in pb(rdReprs, message=\"Generating random scores\", logger=logger):\n",
    "    simMatrix = pairwiseCosineSimilarity(current)\n",
    "    simRankScores.append(pairwiseSimNDCG(simMatrix, indexLabels))\n",
    "    calharScores.append(metrics.calinski_harabasz_score(current, indexLabels))\n",
    "    davbScores.append(metrics.davies_bouldin_score(current, indexLabels))\n",
    "bp(simRankScores, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "model = \"random\"\n",
    "meta = {'dimensions': dimensions, 'rdAmount': rdAmount}\n",
    "# We compute and store score:\n",
    "addResult(model, 'simrank', float(np.mean(simRankScores)), **meta)\n",
    "addResult(model, 'calhar', float(np.mean(calharScores)), **meta)\n",
    "addResult(model, 'davb', float(np.mean(davbScores)), **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Random done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6ebdd3e05d4388c658ca2d5c53b0bc36 (filtered_sentences)\n",
    "# 7fa22619af09e90724d2e3a8cf5db796 (sentences)\n",
    "# 94cdfee38dc865ccc73f2eab5b1fb3a5 (LSTM filtered_sentences)\n",
    "deepStyleModels = \\\n",
    "{\n",
    "    \"6ebdd3e05d4388c658ca2d5c53b0bc36\": \"sna\",\n",
    "    # \"7fa22619af09e90724d2e3a8cf5db796\": \"sna-nf\",\n",
    "    \"94cdfee38dc865ccc73f2eab5b1fb3a5\": \"lstm\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelPattern, name in deepStyleModels.items():\n",
    "    m = DeepStyle(nosaveDir() + \"/asa2-train/\" + modelPattern, logger=logger)\n",
    "    embeddings = np.array([m.embed(doc) for doc in docs])\n",
    "    # We set vars:\n",
    "    data = embeddings\n",
    "    model = name\n",
    "    meta = {'pattern': modelPattern}\n",
    "    # We compute and store score:\n",
    "    simMatrix = pairwiseCosineSimilarity(data)\n",
    "    score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "    addResult(model, 'simrank', score, **meta)\n",
    "    score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "    addResult(model, 'calhar', score, **meta)\n",
    "    score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "    addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get random scores:\n",
    "randomScores = []\n",
    "for i in range(randomTestsAmount):\n",
    "    randomScores.append(pairwiseSimNDCG(simMatrix, shuffle(indexLabels)))\n",
    "randomScore = np.mean(randomScores) if len(randomScores) > 0 else None\n",
    "log(\"Shuffled labels mean SimRank: \" + str(randomScore), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"DeepStyle done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getD2v(path, loadModel=True):\n",
    "    d2vConfig = None\n",
    "    d2vModel = None\n",
    "    if isFile(path + \"/config.json\"):\n",
    "        d2vConfig = fromJsonFile(path + \"/config.json\")\n",
    "    if loadModel:\n",
    "        d2vModel = Doc2Vec.load(sortedGlob(path + \"/*model*.d2v\")[0])\n",
    "    return (d2vModel, d2vConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vPath = nosaveDir() + \"/d2v/d2vmodel-t-ds22.02g-s300-w3-n15-e15-lTrue-adFalse-7bb8a\"\n",
    "# d2vPath = nosaveDir() + \"/d2v/d2vmodel-t-ds6.2g-s300-w3-n30-e15-lTrue-adFalse-4b2d0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the pretrained d2v model:\n",
    "log(\"Loading \" + d2vPath, logger)\n",
    "(d2vModel, d2vConfig) = getD2v(d2vPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get d2v vectors:\n",
    "d2vData = d2vTokenssToEmbeddings(truncatedAndLoweredDocs, d2vModel, logger=logger, verbose=False) # 4m 34.06s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "data = d2vData\n",
    "model = 'doc2vec'\n",
    "meta = {'pattern': d2vPath}\n",
    "# We compute and store score:\n",
    "simMatrix = pairwiseCosineSimilarity(data)\n",
    "score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "addResult(model, 'simrank', score, **meta)\n",
    "score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "addResult(model, 'calhar', score, **meta)\n",
    "score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get random scores:\n",
    "randomScores = []\n",
    "for i in range(randomTestsAmount):\n",
    "    randomScores.append(pairwiseSimNDCG(simMatrix, shuffle(indexLabels)))\n",
    "randomScore = np.mean(randomScores) if len(randomScores) > 0 else None\n",
    "log(\"Shuffled labels mean SimRank: \" + str(randomScore), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vModel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Doc2Vec done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidfInstance = TFIDF(truncatedAndLoweredDocs, doLower=True, logger=logger)\n",
    "tfidfData = tfidfInstance.getTFIDFMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "data = tfidfData\n",
    "model = 'tfidf'\n",
    "meta = {'lowered': True}\n",
    "# We compute and store score:\n",
    "simMatrix = pairwiseCosineSimilarity(data)\n",
    "score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "addResult(model, 'simrank', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"TFIDF done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponentss = [10, 50, 100, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nComponents in nComponentss:\n",
    "    svd = TruncatedSVD(n_components=nComponents, n_iter=7, random_state=42)\n",
    "    svdTFIDFData = svd.fit_transform(tfidfData)\n",
    "    # We set vars:\n",
    "    data = svdTFIDFData\n",
    "    model = 'tfidf+svd-' + str(nComponents)\n",
    "    meta = {'lowered': True, 'nComponents': nComponents}\n",
    "    # We compute and store score:\n",
    "    simMatrix = pairwiseCosineSimilarity(data)\n",
    "    score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "    addResult(model, 'simrank', score, **meta)\n",
    "    score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "    addResult(model, 'calhar', score, **meta)\n",
    "    score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "    addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"TFIDF+SVD done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styloVectors = []\n",
    "for text in pb(detokDocs, logger=logger):\n",
    "    styloVectors.append(stylo(text, asNpArray=True))\n",
    "styloVectors = np.array(styloVectors)\n",
    "bp(styloVectors, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "data = styloVectors\n",
    "model = 'stylo'\n",
    "meta = {}\n",
    "# We compute and store score:\n",
    "simMatrix = pairwiseCosineSimilarity(data)\n",
    "score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "addResult(model, 'simrank', score, **meta)\n",
    "score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "addResult(model, 'calhar', score, **meta)\n",
    "score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Stylo done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvPatterns = [\"glove-840B\", \"glove-6B\", \"word2vec-googlenews\", \"fasttext-wiki-news-1M\", \"fasttext-crawl-2M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wvPattern in wvPatterns:\n",
    "    try:\n",
    "        emb = Embeddings(wvPattern, logger=logger, verbose=False)\n",
    "        wordEmbeddings = emb.getVectors()\n",
    "        lowered = emb.isLower()\n",
    "        for operation in ['sum', 'mean']:\n",
    "            wvData = tokensToEmbedding(truncatedDocs, wordEmbeddings, operation=operation,\n",
    "                                       removeDuplicates=True, doLower=lowered, verbose=False)\n",
    "            # We set vars:\n",
    "            data = wvData\n",
    "            model = \"wordvectors-\" + wvPattern + '-' + operation\n",
    "            meta = {'wvPattern': wvPattern, 'operation': operation, 'lowered': lowered}\n",
    "            # We compute and store score:\n",
    "            simMatrix = pairwiseCosineSimilarity(data)\n",
    "            score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "            addResult(model, 'simrank', score, **meta)\n",
    "            score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "            addResult(model, 'calhar', score, **meta)\n",
    "            score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "            addResult(model, 'davb', score, **meta)\n",
    "    except Exception as e:\n",
    "        logException(e, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"WordVectors done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get all embeddgins from usent:\n",
    "allHashes = set()\n",
    "for doc in docs:\n",
    "    docHash = objectToHash(doc)\n",
    "    allHashes.add(docHash)\n",
    "    for sentence in doc:\n",
    "        theHash = objectToHash(sentence)\n",
    "        allHashes.add(theHash)\n",
    "bp(allHashes, logger)\n",
    "usentEmbeddings = dict()\n",
    "for usentEmbeddingsFile in pb(sortedGlob(nosaveDir() + \"/usent/usentEmbedding*.pickle\"),\n",
    "                              printRatio=0.001, logger=logger, message=\"Getting Usent embeddings from all files\"):\n",
    "    current = deserialize(usentEmbeddingsFile)\n",
    "    for theHash, value in current.items():\n",
    "        if theHash in allHashes:\n",
    "            usentEmbeddings[theHash] = value\n",
    "bp(usentEmbeddings, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(allHashes) == len(usentEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\"mean\", \"full\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for operation in operations:\n",
    "    if operation == \"full\":\n",
    "        data = []\n",
    "        for doc in docs:\n",
    "            theHash = objectToHash(doc)\n",
    "            assert theHash in usentEmbeddings\n",
    "            data.append(usentEmbeddings[theHash])\n",
    "        data = np.array(data)\n",
    "    elif operation == \"mean\":\n",
    "        data = []\n",
    "        for doc in docs:\n",
    "            docEmbeddings = []\n",
    "            for sentence in doc:\n",
    "                theHash = objectToHash(sentence)\n",
    "                assert theHash in usentEmbeddings\n",
    "                docEmbeddings.append(usentEmbeddings[theHash])\n",
    "            docEmbedding = np.mean(docEmbeddings, axis=0)\n",
    "            data.append(docEmbedding)\n",
    "        data = np.array(data)\n",
    "    # We set vars:\n",
    "    data = data\n",
    "    model = \"usent-\" + operation\n",
    "    meta = {'operation': operation}\n",
    "    # We compute and store score:\n",
    "    simMatrix = pairwiseCosineSimilarity(data)\n",
    "    score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "    addResult(model, 'simrank', score, **meta)\n",
    "    score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "    addResult(model, 'calhar', score, **meta)\n",
    "    score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "    addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Usent done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best is `V=1` and `method=\"mean\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "bash(\"git clone https://github.com/facebookresearch/InferSent infersent\")\n",
    "bash(\"touch infersent/__init__.py\")\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isDocker():\n",
    "    bash(\"pip install torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infersent.models import InferSent\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = [2, 1]\n",
    "methods = [\"full\", \"mean\"] # [\"full\", \"mean\", \"sum\"]\n",
    "pbar = ProgressBar(len(Vs) * len(methods), printRatio=0.001, logger=logger)\n",
    "for V in Vs:\n",
    "    MODEL_PATH = nosaveDir() + '/infersent/infersent%s.pkl' % V\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "    infersent = InferSent(params_model)\n",
    "    infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "    if V == 2:\n",
    "        W2V_PATH = nosaveDir() + '/infersent/fastText/crawl-300d-2M.vec'\n",
    "    else:\n",
    "        W2V_PATH = nosaveDir() + '/infersent/GloVe/glove.840B.300d.txt'\n",
    "    infersent.set_w2v_path(W2V_PATH)\n",
    "    infersent.build_vocab(detokDocs, tokenize=True)\n",
    "    for method in methods:\n",
    "        if method == \"full\":\n",
    "            detokDocsForInferSent = []\n",
    "            for current in detokDocs:\n",
    "                detokDocsForInferSent.append(current[:10000])\n",
    "            isData = infersent.encode(detokDocsForInferSent, tokenize=True)\n",
    "        elif method == \"mean\":\n",
    "            isData = []\n",
    "            for currentDetokSentences in detokSentences:\n",
    "                embedding = np.mean(infersent.encode(currentDetokSentences, tokenize=True), axis=0)\n",
    "                isData.append(embedding)\n",
    "            isData = np.array(isData)\n",
    "        # We set vars:\n",
    "        data = isData\n",
    "        model = \"infersent-\" + str(V) + '-' + method\n",
    "        meta = {'V': V, 'operation': method}\n",
    "        # We compute and store score:\n",
    "        simMatrix = pairwiseCosineSimilarity(data)\n",
    "        score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "        addResult(model, 'simrank', score, **meta)\n",
    "        score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "        addResult(model, 'calhar', score, **meta)\n",
    "        score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "        addResult(model, 'davb', score, **meta)\n",
    "        pbar.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"InferSent done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertCache = SerializableDict(\"newsid-bretcache\", useMongodb=True, user=user, password=password, host=host, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertData = []\n",
    "for doc in docs:\n",
    "    theHash = objectToHash(doc)\n",
    "    current = bertCache[theHash]\n",
    "    assert current is not None\n",
    "    bertData.append(current)\n",
    "bertData = np.array(bertData)\n",
    "bp(bertData, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set vars:\n",
    "data = bertData\n",
    "model = \"bert\"\n",
    "meta = {}\n",
    "# We compute and store score:\n",
    "simMatrix = pairwiseCosineSimilarity(data)\n",
    "score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "addResult(model, 'simrank', score, **meta)\n",
    "score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "addResult(model, 'calhar', score, **meta)\n",
    "score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "addResult(model, 'davb', score, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt.tic(\"BERT done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Cython ; git clone https://github.com/epfml/sent2vec.git ; cd ./sent2vec ; pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installSent2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelName in [\"wiki_unigrams.bin\"]: # [\"wiki_unigrams.bin\", \"wiki_bigrams.bin\"]\n",
    "    s2vModel = sent2vec.Sent2vecModel()\n",
    "    s2vModel.load_model(nosaveDir() + '/sent2vec/' + modelName)\n",
    "    for method in [\"full\", \"mean\"]: # [\"full\", \"mean\", \"sum\"]\n",
    "        if method == \"full\":\n",
    "            s2vData = s2vModel.embed_sentences(detokDocs)\n",
    "        elif method == \"mean\":\n",
    "            s2vData = []\n",
    "            for currentDetokSentences in detokSentences:\n",
    "                embedding = np.mean(s2vModel.embed_sentences(currentDetokSentences), axis=0)\n",
    "                s2vData.append(embedding)\n",
    "            s2vData = np.array(s2vData)\n",
    "        # We set vars:\n",
    "        data = s2vData\n",
    "        model = \"sent2vec-\" + modelName + '-' + method\n",
    "        meta = {'pattern': modelName, 'operation': method}\n",
    "        # We compute and store score:\n",
    "        simMatrix = pairwiseCosineSimilarity(data)\n",
    "        score = pairwiseSimNDCG(simMatrix, indexLabels)\n",
    "        addResult(model, 'simrank', score, **meta)\n",
    "        score = metrics.calinski_harabasz_score(data, indexLabels)\n",
    "        addResult(model, 'calhar', score, **meta)\n",
    "        score = metrics.davies_bouldin_score(data, indexLabels)\n",
    "        addResult(model, 'davb', score, **meta)\n",
    "    try:\n",
    "        s2vModel.release_shared_mem(nosaveDir() + '/sent2vec/' + modelName)\n",
    "        s2vModel = None\n",
    "    except Exception as e:\n",
    "        logException(e, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Sent2Vec done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
