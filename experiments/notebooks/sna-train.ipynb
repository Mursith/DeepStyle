{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 1:\n",
    "# screen -S asa2-train1\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/asa2-train-logs\n",
    "# DOCKER_PORT=9961 nn -o nohup-asa2-train-$HOSTNAME-1.out ~/docker/keras/run-jupython.sh ~/notebooks/asa/train/asa2-train.ipynb titanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 2:\n",
    "# screen -S asa2-train2\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/asa2-train-logs\n",
    "# DOCKER_PORT=9962 nn -o nohup-asa2-train-$HOSTNAME-2.out ~/docker/keras/run-jupython.sh ~/notebooks/asa/train/asa2-train.ipynb titanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "if isNotebook:\n",
    "    TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasGPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newssource.asattribution.utils import *\n",
    "from newssource.asattribution.asamin import *\n",
    "from newssource.asa.asapreproc import *\n",
    "from newssource.metrics.ndcg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if not isNotebook:\n",
    "    matplotlib.use('Agg')\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning.baseline import *\n",
    "from machinelearning.encoder import *\n",
    "from machinelearning.kerasutils import *\n",
    "from machinelearning.kerasmodels import *\n",
    "from machinelearning.iterator import *\n",
    "from machinelearning.metrics import *\n",
    "from machinelearning.attmap.builder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU, Dense, CuDNNLSTM, CuDNNGRU, Bidirectional\n",
    "from keras.layers import BatchNormalization, Activation, SpatialDropout1D, InputSpec\n",
    "from keras.layers import MaxPooling1D, TimeDistributed, Flatten, concatenate, Conv1D\n",
    "from keras.utils import multi_gpu_model, plot_model\n",
    "from keras.layers import concatenate, Input, Dropout\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import Callback, History, ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    \"patience\": 3 if TEST else 20, # <IMPORTANT>\n",
    "    # 3gramsFiltered, 2gramsFiltered, 1gramsFiltered, textSentences # <IMPORTANT>\n",
    "    # filtered_sentences, sentences\n",
    "    \"dataCol\": \"filtered_sentences\", # <IMPORTANT>\n",
    "    \"wordVectorsPattern\": \"test\" if TEST else \"glove-840B\", # <IMPORTANT>\n",
    "    \"embeddingsDimension\": 100 if TEST else 300,\n",
    "    \"minVocDF\": 2, # 10 if isNotebook else 2,\n",
    "    \"minVocLF\": 2,\n",
    "    \"minTokensLength\": 3,\n",
    "    \"docLength\": 1200, # The size of documents representation (median * 3 = 600)\n",
    "    \"isTrainableEmbeddings\": False,\n",
    "    \"doMultiGPU\": False, # not isNotebook\n",
    "    \"batchSize\": 32 if TEST else 128, # 128, 256 # <IMPORTANT>\n",
    "    \"epochs\": 10 if TEST else 1000,\n",
    "    \"maxQueueSize\": 10 if TEST else 10,\n",
    "    \"saveFinalModel\": False,\n",
    "    \"doNotif\": True, # not isNotebook,\n",
    "    \"attention\": False, # <IMPORTANT>\n",
    "    \"bidirectional\": False, # <IMPORTANT>\n",
    "    \"isCuDNN\": True, # <IMPORTANT>\n",
    "    \"saveMetrics\":\n",
    "    {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_acc\": \"max\",\n",
    "        \"val_top_k_categorical_accuracy\": \"max\",\n",
    "    },\n",
    "    \"metrics\": ['accuracy', 'top_k_categorical_accuracy'], # ['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'], ['accuracy', 'top_k_categorical_accuracy']\n",
    "    \"loss\": 'categorical_crossentropy', # sparse_categorical_crossentropy, categorical_crossentropy\n",
    "    \"trainStepDivider\": 80 if TEST else 0, # 5 à la base......\n",
    "    \"infiniteBatcherShuffle\": 0 if TEST else 0,\n",
    "    \n",
    "    \"inputEncoding\": \"embedding\", # index, embedding\n",
    "    \"labelEncoding\": 'onehot', # onehot, index\n",
    "    \n",
    "    \"denseUnits\": [500, 100], # [100], [500, 100]\n",
    "    \"rnnUnits\": 500,\n",
    "    \n",
    "    \"persist\": [False, False] if TEST else [False, False],\n",
    "}\n",
    "if config[\"isCuDNN\"]:\n",
    "    del config[\"isCuDNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config[\"hostname\"] = getHostname()\n",
    "print(\"hostname: \" + str(getHostname()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDirRoot = nosaveDir() + \"/asa2-train\"\n",
    "print(\"outputDirRoot: \" + outputDirRoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"trainPattern\"] = \"train\" # <IMPORTANT>\n",
    "config[\"validationPattern\"] = \"validation\" # <IMPORTANT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory = dataDir() + \"/Asa2/kset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"outputDir\"] = outputDirRoot + \"/\" + objectToHash(config)\n",
    "mkdir(config[\"outputDir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(config[\"outputDir\"] + \"/asa2-train.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toJsonFile(toMongoStorable(config), config[\"outputDir\"] + \"/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(lts(config), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embeddings(config[\"wordVectorsPattern\"], config[\"embeddingsDimension\"], verbose=True, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEmbeddings = emb.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the embedings dimension:\n",
    "embeddingsDimension = len(wordEmbeddings[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"We loaded word embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFilesPath = sortedGlob(dataDirectory + \"/\" + config[\"trainPattern\"] + \"/*.bz2\")\n",
    "validationFilesPath = sortedGlob(dataDirectory + \"/\" + config[\"validationPattern\"] + \"/*.bz2\")\n",
    "assert len(trainFilesPath) > 0\n",
    "log(\"trainFilesPath:\\n\" + reducedLTS(trainFilesPath, 4), logger)\n",
    "log(\"validationFilesPath:\\n\" + reducedLTS(validationFilesPath, 4), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We search an amount of batches to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInfiniteBatcherSkip = 0\n",
    "# In case we reume a previous train:\n",
    "if len(sortedGlob(config[\"outputDir\"] + \"/models/ep*\")) > 0:\n",
    "    lastEpochPath = sortedGlob(config[\"outputDir\"] + \"/models/ep*\")[-1]\n",
    "    log(\"We found an epoch to resume: \" + lastEpochPath, logger)\n",
    "    batchesPassedFile = lastEpochPath + \"/batchesPassed.txt\"\n",
    "    if isFile(batchesPassedFile):\n",
    "        trainInfiniteBatcherSkip = int(fileToStr(batchesPassedFile))\n",
    "        logWarning(\"We will skip \" + str(trainInfiniteBatcherSkip) + \" batches because we resume a previous train\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt = None\n",
    "prebuiltPath = config[\"outputDir\"] + \"/asap-prebuilt.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isFile(prebuiltPath):\n",
    "    prebuilt = prebuiltPath\n",
    "    log(\"We found \" + prebuilt, logger)\n",
    "else:\n",
    "    log(\"We didn't found any asap prebuilt pickle file...\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We init AsaPreproc:\n",
    "asap = buildASAP\\\n",
    "(\n",
    "    trainFilesPath,\n",
    "    validationFilesPath,\n",
    "    config[\"dataCol\"],\n",
    "    \n",
    "    minTokensLength=config[\"minTokensLength\"],\n",
    "\n",
    "    batchSize=config[\"batchSize\"],\n",
    "    minVocDF=config[\"minVocDF\"],\n",
    "    minVocLF=config[\"minVocLF\"],\n",
    "\n",
    "    wordEmbeddings=wordEmbeddings,\n",
    "    \n",
    "    persist=config[\"persist\"],\n",
    "    \n",
    "    docLength=config[\"docLength\"],\n",
    "    \n",
    "    prebuilt=prebuilt,\n",
    "    \n",
    "    logger=logger,\n",
    "    verbose=True,\n",
    "    \n",
    "    labelEncoding=config[\"labelEncoding\"],\n",
    "    encoding=config[\"inputEncoding\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We serialize all:\n",
    "asap.serializePrebuilt(prebuiltPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"All train and validation data are ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialEpoch = 0\n",
    "if len(sortedGlob(config[\"outputDir\"] + \"/models/ep*\")) > 0:\n",
    "    logWarning(\"#\" * 20 + \" We will resume a previous train \" + \"#\" * 20, logger)\n",
    "    lastEpochPath = sortedGlob(config[\"outputDir\"] + \"/models/ep*\")[-1]\n",
    "    initialEpoch = getFirstNumber(decomposePath(lastEpochPath)[1]) + 1\n",
    "    assert not isFile(config[\"outputDir\"] + \"/finished\")\n",
    "    # To load the model we first build it:\n",
    "    modelKwargs = fromJsonFile(lastEpochPath + \"/kwargs.json\")\n",
    "    assert modelKwargs[\"docLength\"] == asap.getDocLength()\n",
    "    assert modelKwargs[\"vocSize\"] == len(asap.getVocIndex())\n",
    "    assert modelKwargs[\"nbClasses\"] == len(asap.getLabelEncoder())\n",
    "    if \"embeddingsDimension\" not in modelKwargs:\n",
    "        modelKwargs[\"embeddingsDimension\"] = asap.getEmbeddingsDimension() if config[\"inputEncoding\"] == \"embedding\" else None\n",
    "    if \"embeddingMatrix\" not in modelKwargs:\n",
    "        modelKwargs[\"embeddingMatrix\"] = asap.getEmbeddingMatrix() if config[\"inputEncoding\"] == \"index\" else None\n",
    "    (originalModel, modelKwargs, modelScript) = buildRNN\\\n",
    "    (\n",
    "        logger=logger,\n",
    "        verbose=True,\n",
    "        **modelKwargs,\n",
    "    )\n",
    "    # Then we load weights:\n",
    "    if isFile(lastEpochPath + \"/model.h5\"):\n",
    "        originalModel = load_model(lastEpochPath + \"/model.h5\")\n",
    "    else:\n",
    "        originalModel.load_weights(lastEpochPath + \"/weights.h5\") # WARNING we loose optimizer states\n",
    "        # Finally we compile it:\n",
    "        originalModel.compile(loss=config[\"loss\"], optimizer=opt, metrics=config[\"metrics\"])\n",
    "    parallelModel = None\n",
    "    model = originalModel\n",
    "    tt.tic(\"We loaded the model to resume training. Initial epoch: \" + str(initialEpoch), logger)\n",
    "else:\n",
    "    (originalModel, modelKwargs, modelScript) = buildRNN\\\n",
    "    (\n",
    "        docLength=asap.getDocLength(),\n",
    "        vocSize=len(asap.getVocIndex()),\n",
    "        nbClasses=len(asap.getLabelEncoder()),\n",
    "        isEmbeddingsTrainable=config[\"isTrainableEmbeddings\"],\n",
    "        \n",
    "        denseUnits=config[\"denseUnits\"],\n",
    "        rnnUnits=config[\"rnnUnits\"],\n",
    "        \n",
    "        embSpacialDropout=0.2,\n",
    "        firstDropout=0.2,\n",
    "        recurrentDropout=0.2,\n",
    "        attentionDropout=0.2,\n",
    "        denseDropout=0.2,\n",
    "        useRNNDropout=True,\n",
    "        \n",
    "        isBidirectional=config[\"bidirectional\"],\n",
    "        \n",
    "        isCuDNN='isCuDNN' not in config or config[\"isCuDNN\"],\n",
    "        \n",
    "        rnnType='LSTM',\n",
    "        addAttention=config[\"attention\"],\n",
    "        \n",
    "        bnAfterEmbedding=False,\n",
    "        bnAfterRNN=False,\n",
    "        bnAfterAttention=False,\n",
    "        bnAfterDenses=False,\n",
    "        bnAfterLast=False,\n",
    "        bnBeforeActivation=True,\n",
    "\n",
    "        embeddingMatrix=asap.getEmbeddingMatrix() if config[\"inputEncoding\"] == \"index\" else None,\n",
    "        logger=logger,\n",
    "        verbose=True,\n",
    "        \n",
    "        embeddingsDimension=asap.getEmbeddingsDimension() if config[\"inputEncoding\"] == \"embedding\" else None,\n",
    "    )\n",
    "    parallelModel = None\n",
    "    model = originalModel\n",
    "    model.compile(loss=config[\"loss\"], optimizer=opt, metrics=config[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strToFile(originalModel.to_json(), config[\"outputDir\"] + \"/model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We define metrics callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mainCallback = KerasCallback\\\n",
    "(\n",
    "    originalModel,\n",
    "    logger=logger,\n",
    "    graphDir=config[\"outputDir\"] + \"/graphs\",\n",
    "    modelsDir=config[\"outputDir\"] + \"/models\",\n",
    "    doNotif=False,\n",
    "    saveMetrics=config[\"saveMetrics\"],\n",
    "    doPltShow=isNotebook,\n",
    "    historyFile=config[\"outputDir\"] + \"/history.json\",\n",
    "    initialEpoch=initialEpoch,\n",
    "    stopFile=config[\"outputDir\"] + \"/stop\",\n",
    "    earlyStopMonitor=\\\n",
    "    {\n",
    "        'val_loss': {'patience': config[\"patience\"]},\n",
    "        'val_acc': {'patience': config[\"patience\"]},\n",
    "        'val_top_k_categorical_accuracy': {'patience': config[\"patience\"]},\n",
    "    },\n",
    "    batchesPassed=trainInfiniteBatcherSkip,\n",
    "    batchesAmount=asap.getBatchesCount(0),\n",
    "    \n",
    "    saveFunct=saveModel,\n",
    "    saveFunctKwargs=\\\n",
    "    {\n",
    "        \"makeSubDir\": False, \"kwargs\": modelKwargs, \"script\": modelScript,\n",
    "        \"extraInfos\":\\\n",
    "        {\n",
    "            \"wordVectorsPattern\": config[\"wordVectorsPattern\"],\n",
    "            \"embeddingsDimension\": config[\"embeddingsDimension\"],\n",
    "            \"minVocDF\": config[\"minVocDF\"],\n",
    "            \"minVocLF\": config[\"minVocLF\"],\n",
    "            \"dataCol\": config[\"dataCol\"],\n",
    "            \"labelEncoding\": config[\"labelEncoding\"],\n",
    "            \"inputEncoding\": config[\"inputEncoding\"],\n",
    "            \"trainStepDivider\": config[\"trainStepDivider\"],\n",
    "            \"infiniteBatcherShuffle\": config[\"infiniteBatcherShuffle\"],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsPerEpoch = asap.getBatchesCount(0)\n",
    "if dictContains(config, \"trainStepDivider\") and config[\"trainStepDivider\"] > 1:\n",
    "    stepsPerEpoch = math.ceil(stepsPerEpoch / config[\"trainStepDivider\"])\n",
    "    log(\"The stepsPerEpoch was \" + str(asap.getBatchesCount(0)) + \" but now is \" + str(stepsPerEpoch), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log(\"We launch fit_generator\", logger)\n",
    "asap.verbose = True # if TEST else False\n",
    "history = model.fit_generator\\\n",
    "(\n",
    "    asap.getInfiniteBatcher(0, shuffle=config[\"infiniteBatcherShuffle\"], skip=trainInfiniteBatcherSkip),\n",
    "    steps_per_epoch=stepsPerEpoch,\n",
    "    validation_data=asap.getInfiniteBatcher(1),\n",
    "    validation_steps=asap.getBatchesCount(1) / 40 if TEST else asap.getBatchesCount(1),\n",
    "    epochs=config[\"epochs\"],\n",
    "    verbose=1,\n",
    "    max_queue_size=config[\"maxQueueSize\"],\n",
    "    callbacks=[mainCallback, callbacks.TerminateOnNaN()],\n",
    "    initial_epoch=initialEpoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainCallback.logHistory()\n",
    "log(\"Best val_loss: \" + str(min(mainCallback.history[\"val_loss\"])), logger)\n",
    "log(\"Best val_acc: \" + str(max(mainCallback.history[\"val_acc\"])), logger)\n",
    "log(\"Best val_top_k_acc: \" + str(max(mainCallback.history[\"val_top_k_categorical_accuracy\"])), logger)\n",
    "log(\"Nb epochs: \" + str(len(mainCallback.history[\"val_loss\"])), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isNotebook:\n",
    "    notif(\"Training done\", lts(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We save the model and infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # toJsonFile(history.history, config[\"outputDir\"] + \"/history.json\")\n",
    "    toJsonFile({\"epochs\": mainCallback.epochs, \"history\": mainCallback.history},\n",
    "               config[\"outputDir\"] + \"/history.json\")\n",
    "except Exception as e:\n",
    "    logException(e, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"saveFinalModel\"]:\n",
    "    finalModelDirectory = config[\"outputDir\"] + \"/final-model\"\n",
    "    mkdir(finalModelDirectory)\n",
    "    originalModel.save(finalModelDirectory + '/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "touch(config[\"outputDir\"] + \"/finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    asap.setParallelProcesses(1) # For consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    # We get val encoded tokens:\n",
    "    valEncodedTokens = np.array([tokens for tokens, encodedAd in asap.getPart(1)])\n",
    "    # We get documents non-lowered and non-flattened, with masks (pre-padding):\n",
    "    valPaddedDocs = [tokens for tokens, _ in asap.getRawPart(1, pad=True)]\n",
    "    # We get all encoded labels:\n",
    "    valEncodedLabels = np.array([encodedAd for tokens, encodedAd in asap.getPart(1)])\n",
    "    # We get all labels:\n",
    "    valLabels = [ad for tokens, ad in asap.getRawPart(1)]\n",
    "    # We get the dict label -> encodedLabel (we can also use asap.encodedLabelToLabel(encodedLabel)):\n",
    "    encodedAds = asap.getLabelEncoder()\n",
    "    # We display it:\n",
    "    bp(valPaddedDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    # We get predictions:\n",
    "    predictionsAsSoftmax = model.predict(valEncodedTokens)\n",
    "    # We convert it to encoded labels according to the max probability of softmax vectors:\n",
    "    predictionsAsEncodedLabel = []\n",
    "    for i in range(len(predictionsAsSoftmax)):\n",
    "        predSoftmax = predictionsAsSoftmax[i]\n",
    "        predEncodedLabel = np.zeros(len(predSoftmax))\n",
    "        predEncodedLabel[np.argmax(predSoftmax)] = 1\n",
    "        predictionsAsEncodedLabel.append(predEncodedLabel)\n",
    "    # And we convert all to labels:\n",
    "    predictionsAsLabel = [asap.decodeLabel(enc) for enc in predictionsAsEncodedLabel]\n",
    "    # We display it:\n",
    "    bp(predictionsAsLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    # We get attentions:\n",
    "    attentions = getAttentions(model, valEncodedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    # Now we compute the accuracy:\n",
    "    wellClassifiedCount = 0\n",
    "    for i in range(len(predictionsAsLabel)):\n",
    "        if valLabels[i] == predictionsAsLabel[i]:\n",
    "            wellClassifiedCount += 1\n",
    "    print(\"Accuracy: \" + str(truncateFloat(wellClassifiedCount / len(predictionsAsLabel) * 100.0, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    # And finally we print some attentions:\n",
    "    for i in range(10, 50):\n",
    "        doc = valPaddedDocs[i]\n",
    "        label = valLabels[i]\n",
    "        predLabel = predictionsAsLabel[i]\n",
    "        okToken = \"==> OK <==\" if label == predLabel else \"==> FAIL <==\"\n",
    "        print(okToken + \" Prediction: \" + label[:30] + \", Ground truth: \" + str(predLabel)[:30])\n",
    "        attention = attentions[i]\n",
    "        showAttentionMap(doc, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"doNotif\"]:\n",
    "    notif(\"LARGE training done on \" + getHostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
